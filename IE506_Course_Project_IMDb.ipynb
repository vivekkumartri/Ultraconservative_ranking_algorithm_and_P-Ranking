{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "exact-nebraska",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/22n0457/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.utils.extmath import safe_sparse_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "registered-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_sup_df=pd.read_csv(\"/home/22n0457/imdb_sup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "considerable-brother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kurt Russell's chameleon-like performance, cou...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was extremely low budget(it some scenes it ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James Cagney is best known for his tough chara...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Following the brilliant \"Goyôkiba\" (aka. \"Hanz...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of the last classics of the French New Wav...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Having just watched this film again from a 199...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Straight Story is a truly beautiful movie ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Four teenage girlfriends drive to Fort Laurdal...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I haven't seen all of Jess Franco's movies, I ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What's in a name? If the name is Jerry Bruckhe...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  Sentiment\n",
       "0  Kurt Russell's chameleon-like performance, cou...      10          1\n",
       "1  It was extremely low budget(it some scenes it ...       8          1\n",
       "2  James Cagney is best known for his tough chara...       8          1\n",
       "3  Following the brilliant \"Goyôkiba\" (aka. \"Hanz...       8          1\n",
       "4  One of the last classics of the French New Wav...      10          1\n",
       "5  Having just watched this film again from a 199...      10          1\n",
       "6  The Straight Story is a truly beautiful movie ...      10          1\n",
       "7  Four teenage girlfriends drive to Fort Laurdal...       7          1\n",
       "8  I haven't seen all of Jess Franco's movies, I ...       9          1\n",
       "9  What's in a name? If the name is Jerry Bruckhe...       7          1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_sup_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "irish-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_sup_df=imdb_sup_df.iloc[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "severe-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(message):\n",
    "  '''\n",
    "  message = \"#'This', is $string #with punction, <br /> @'html_tag' and actual message also!\"\n",
    "\n",
    "  return 'string punction html_tag actual message also'\n",
    "  '''\n",
    "\n",
    "  html_tag = '<br />'\n",
    "  message = message.replace(html_tag,'')  # remove html tag\n",
    "  message = re.sub(r'[^\\w\\s]', '', message)   # remove punctiation\n",
    "  message = message.lower()\n",
    "  message = [word for word in message.split() if word not in stopwords.words('english')]\n",
    "  message = ' '.join(message)\n",
    "\n",
    "  return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "plain-williams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string punction html_tag actual message also'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"#'This', is $string #with punction, <br /> @'html_tag' and actual message also!\"\n",
    "text_clean(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "informal-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_sup_df['Review'] = imdb_sup_df['Review'].apply(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "provincial-logistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kurt russells chameleonlike performance couple...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>extremely low budgetit scenes looks like recor...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>james cagney best known tough characters gangs...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>following brilliant goyôkiba aka hanzo razor s...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one last classics french new wave direction ci...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>watching midnight cowboy like taking mastercla...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ladies man suffers common problem among movies...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>picked movie cover even knowing watched laughe...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>movie awesome nonstop laugh riot incorporating...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>jane eyre always favorite novel stumbled upon ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  Rating  Sentiment\n",
       "0    kurt russells chameleonlike performance couple...      10          1\n",
       "1    extremely low budgetit scenes looks like recor...       8          1\n",
       "2    james cagney best known tough characters gangs...       8          1\n",
       "3    following brilliant goyôkiba aka hanzo razor s...       8          1\n",
       "4    one last classics french new wave direction ci...      10          1\n",
       "..                                                 ...     ...        ...\n",
       "995  watching midnight cowboy like taking mastercla...      10          1\n",
       "996  ladies man suffers common problem among movies...       7          1\n",
       "997  picked movie cover even knowing watched laughe...      10          1\n",
       "998  movie awesome nonstop laugh riot incorporating...       7          1\n",
       "999  jane eyre always favorite novel stumbled upon ...      10          1\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_sup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "qualified-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porterstemmer = PorterStemmer()\n",
    "\n",
    "def steming(message):\n",
    "  return[porterstemmer.stem(word) for word in message.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "owned-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 minute\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=steming) # here stemming is our user defined function as above \n",
    "\n",
    "x = tfidf.fit_transform(imdb_sup_df['Review']).toarray()\n",
    "y = imdb_sup_df['Rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "heavy-clock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8,  8,  8, 10, 10, 10,  7,  9,  7,  7, 10, 10, 10,  7,  8, 10,\n",
       "       10,  8,  7,  8, 10,  9,  8,  8,  9,  9,  8, 10, 10,  9,  7,  7, 10,\n",
       "        8,  9,  9,  8,  7, 10,  8,  8,  9,  9, 10, 10,  9,  8, 10, 10,  7,\n",
       "        9, 10, 10,  8,  8, 10,  8,  8, 10,  9, 10,  9, 10,  7,  7, 10,  7,\n",
       "        9,  7,  7, 10, 10,  8,  8, 10,  8, 10, 10,  8,  8,  8,  9,  8,  8,\n",
       "        7, 10,  8,  7,  8,  7, 10, 10, 10,  9,  8, 10,  7,  9,  7,  7,  7,\n",
       "        8, 10,  8,  8,  7,  7, 10,  9,  7, 10,  8, 10,  9,  8, 10, 10,  8,\n",
       "        7,  8,  8,  9,  7, 10,  8, 10, 10,  7, 10,  8,  9,  9, 10,  9, 10,\n",
       "       10,  9,  7, 10,  9,  9,  7, 10, 10,  7,  9, 10,  8,  8,  7,  7,  8,\n",
       "        8,  8, 10,  8,  7,  7,  7, 10,  9,  8, 10,  7, 10,  7,  7, 10,  9,\n",
       "        8,  7,  9,  7, 10,  8, 10, 10, 10,  7,  8,  7,  8,  9,  7,  8,  9,\n",
       "       10,  9, 10,  9,  9,  9,  7, 10,  7,  8, 10, 10,  7, 10,  8,  8, 10,\n",
       "       10,  7,  8,  7,  7, 10,  8,  9, 10,  7, 10,  8,  8,  9,  7,  8,  9,\n",
       "       10, 10,  7, 10, 10,  7, 10,  8, 10, 10, 10,  9, 10,  8,  9,  8, 10,\n",
       "        7, 10,  7,  8, 10,  8,  7, 10,  8,  7, 10,  8,  7,  7, 10,  9, 10,\n",
       "        8,  8,  7,  7, 10,  8,  8, 10, 10, 10,  8,  8, 10,  8,  8,  7, 10,\n",
       "        9, 10,  8, 10,  9, 10,  7,  8, 10, 10,  8,  8, 10, 10,  7,  7, 10,\n",
       "       10, 10,  8,  9,  7,  7,  9,  9,  7,  8,  8,  7,  9, 10,  7,  8,  8,\n",
       "       10, 10,  7,  7, 10, 10,  7,  9, 10,  9, 10,  9,  8, 10,  7,  9,  9,\n",
       "       10, 10,  9, 10, 10,  7,  7,  8,  9, 10,  8, 10,  7, 10,  9,  8,  7,\n",
       "       10,  8,  7,  8,  7,  8,  7,  7,  8, 10,  7,  8,  7,  8, 10, 10,  9,\n",
       "       10,  7, 10,  8, 10,  7, 10,  7, 10, 10,  9, 10,  9, 10,  8,  7,  9,\n",
       "        7,  9,  7,  7,  7,  9, 10,  8,  7,  7,  9,  8, 10,  9,  9,  8,  9,\n",
       "        7, 10, 10,  8,  7,  7, 10, 10, 10,  9,  9, 10,  9,  8, 10,  8, 10,\n",
       "       10,  7,  7,  8,  8,  7,  7, 10, 10,  9,  8,  8,  9,  7,  8,  9,  7,\n",
       "        7,  9,  7, 10,  8,  7,  8,  8,  7,  7,  7,  9,  9,  8, 10, 10,  9,\n",
       "       10, 10,  8, 10,  9,  8, 10, 10,  9,  8, 10, 10, 10,  7,  8,  8,  9,\n",
       "        8, 10, 10,  9, 10, 10,  9,  7, 10, 10, 10, 10, 10,  7,  9, 10,  8,\n",
       "        7,  8, 10, 10,  8,  8, 10,  7,  8,  8, 10, 10, 10, 10,  8, 10, 10,\n",
       "        7,  7,  8,  7,  7, 10, 10,  9,  9, 10,  9,  9,  8,  8,  8, 10,  9,\n",
       "        8, 10,  9, 10, 10,  7,  7, 10, 10, 10,  9,  8,  9, 10, 10, 10,  8,\n",
       "       10, 10,  8, 10,  7,  7,  8, 10,  7,  7,  9, 10,  8,  8,  7,  7,  9,\n",
       "        9,  9,  9, 10, 10,  8,  9, 10,  7,  9, 10,  7, 10,  9,  8,  8,  7,\n",
       "        8, 10, 10,  9,  8, 10,  8,  7,  8, 10, 10, 10, 10, 10,  7,  8,  7,\n",
       "       10,  9,  7,  9, 10,  8, 10,  8,  8, 10,  8,  7, 10, 10, 10, 10,  9,\n",
       "        9, 10,  8, 10,  7, 10, 10, 10,  7,  7, 10, 10,  7,  8, 10,  8,  7,\n",
       "       10, 10, 10, 10,  8,  8,  7,  8,  8,  7, 10,  7,  8, 10,  9,  7,  7,\n",
       "       10, 10,  9,  8,  7,  9, 10, 10,  7,  7,  9, 10,  7,  9,  9,  9, 10,\n",
       "       10,  9,  8, 10, 10,  7,  9, 10,  9,  7,  7,  9, 10,  8,  9,  8,  7,\n",
       "        8,  8,  7,  7,  9,  9, 10, 10,  8, 10,  9,  8,  7, 10,  8, 10,  8,\n",
       "        8,  9,  9,  9,  7,  8, 10,  9, 10,  7, 10,  9,  8, 10,  8,  7,  7,\n",
       "        8,  7,  7,  7,  8,  9, 10, 10,  8,  8,  7,  9,  8,  8, 10, 10,  9,\n",
       "        8,  8, 10,  9,  8, 10,  7, 10,  8,  8,  8, 10,  8,  8,  7, 10, 10,\n",
       "        8,  9,  8,  7,  7,  9,  7,  9, 10, 10, 10, 10,  7,  7,  7,  7,  8,\n",
       "       10, 10,  7,  9, 10,  9, 10, 10,  9,  9,  8,  8, 10,  9, 10,  7, 10,\n",
       "        8, 10, 10, 10, 10,  8, 10, 10,  9,  8,  8,  8,  9, 10,  8, 10,  8,\n",
       "        7,  9,  9,  7,  7,  7,  7,  7,  9,  8, 10,  8, 10, 10, 10,  7,  8,\n",
       "        8,  8, 10, 10,  8, 10, 10, 10,  9, 10,  8, 10,  7, 10,  8, 10, 10,\n",
       "       10,  7,  9,  8, 10,  8,  7,  7,  8, 10, 10,  9, 10,  8, 10,  9, 10,\n",
       "        8, 10, 10, 10,  8,  7,  8,  8, 10,  8,  8, 10, 10,  7,  8, 10, 10,\n",
       "       10,  9,  8,  7,  8,  7, 10,  8,  8,  7,  8, 10, 10,  8,  9, 10,  8,\n",
       "        9, 10, 10,  8,  7, 10,  8,  8,  8, 10,  8, 10,  8,  8,  7,  9, 10,\n",
       "       10,  7,  7,  9,  7,  9,  8,  9, 10, 10,  9,  9,  7,  7, 10,  8, 10,\n",
       "        7,  8,  7,  8,  7, 10,  9,  7,  7,  7,  8,  9,  8,  9,  9, 10,  7,\n",
       "        7, 10,  7,  8, 10, 10, 10, 10,  7, 10,  9, 10,  9,  8,  7,  8, 10,\n",
       "        9,  9,  8,  8,  8,  7, 10,  7, 10,  7,  9,  8,  8,  7,  9,  9,  8,\n",
       "        8, 10,  8,  7,  8, 10,  9,  8,  8, 10, 10,  7,  7,  7,  9, 10,  8,\n",
       "       10,  7,  9, 10, 10, 10, 10, 10,  8,  9, 10, 10,  9,  9,  8, 10,  9,\n",
       "        8,  9,  8,  9,  7,  8, 10, 10,  8, 10,  7, 10,  7, 10])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "selective-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def widrowhoff(train,y,t,c,m):\n",
    "    w=np.array([0 for i in range(np.shape(train)[1])])\n",
    "    loss=[]\n",
    "    label=range(1,len(np.unique(y))+1)\n",
    "    l2=np.sort(np.unique(y))\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(label)):\n",
    "            if y[i]==l2[j]:\n",
    "                y[i]=label[j]\n",
    "    for s in range(m):\n",
    "        for i in range(1,t+1):\n",
    "            x=np.array(train[i-1])\n",
    "            y_bart=safe_sparse_dot(w,x)\n",
    "            #if i%1000==0:\n",
    "            if i%2==0:\n",
    "                y_bar=[]\n",
    "                for t in range(1,i+1):\n",
    "                    x=np.array(train[t-1])\n",
    "                    y_bart=safe_sparse_dot(w,x)\n",
    "                    y_bar.append(y_bart)\n",
    "                l=sum([np.absolute(y[j-1]-y_bar[j-1]) for j in range(1,i+1)])/i\n",
    "                print(f\"loss in round {i} is \",l)\n",
    "                loss.append(l)\n",
    "            w=np.subtract(w,c*(safe_sparse_dot(w,x)-y[i-1])*x)\n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "general-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pranking(train,y,t,m):\n",
    "    w=np.zeros(train.shape[1])\n",
    "    b = np.zeros(len(np.unique(y)) - 1)\n",
    "    b = np.append(b, np.inf)\n",
    "    loss=[]\n",
    "    label=range(1,len(np.unique(y))+1)\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    y = np.array([np.where(unique_y == labels)[0][0] + 1 for labels in y]) \n",
    "    def predict(x, w, b):\n",
    "        scores = safe_sparse_dot(x, w) < b\n",
    "        return np.argmax(scores) + 1\n",
    "    for s in range(m):\n",
    "        for i in range(1,t+1):\n",
    "            x=train[i-1]\n",
    "            y_bar=predict(x, w, b)\n",
    "            if y_bar !=y[i-1]:\n",
    "                y_rt=np.where(y[i-1]<=label[:-1],-1,1)\n",
    "                t_rt=np.where((safe_sparse_dot(x,w)-b[:-1])*y_rt <=0,y_rt,0)\n",
    "                w=w+sum(t_rt)*x\n",
    "                b[:-1] -= t_rt\n",
    "            #predicting score\n",
    "            #if i%1000==0:\n",
    "            if i%2==0:\n",
    "                y_bar1=np.array([predict(train[j-1],w,b) for j in range(1,i+1)])\n",
    "                l = np.mean(np.abs(y[:i] - y_bar1)) #loss function\n",
    "                print(f\"loss in round {i} is \",l)\n",
    "                loss.append(l)\n",
    "    return loss,w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "annual-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniformmulticlassalgo(feature,y,maxiter,d):\n",
    "    m=np.zeros((len(np.unique(y)),len(feature[0])))\n",
    "    loss=[]\n",
    "    def taugenerate(h,e,yt):\n",
    "        t=[0 for i in range(h)]\n",
    "        t[int(yt-1)]=1\n",
    "        for k in e:\n",
    "            t[k-1]=-(1/len(e))\n",
    "        return t\n",
    "    label=range(1,len(np.unique(y))+1)\n",
    "    l2=np.sort(np.unique(y))\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(label)):\n",
    "            if y[i]==l2[j]:\n",
    "                y[i]=label[j]\n",
    "    for s in range(d):\n",
    "        for i in range(1,maxiter+1):\n",
    "            x=feature[i-1]\n",
    "            ybart=np.argmax([safe_sparse_dot(m[j],x) for j in range(len(label))])+1\n",
    "            yt=y[i-1]\n",
    "            e=[]\n",
    "            for r in range(1,len(label)+1):\n",
    "                if  r!=yt:\n",
    "                    if safe_sparse_dot(m[int(r-1)],x)>=safe_sparse_dot(m[int(yt-1)],x):\n",
    "                        e.append(r)\n",
    "            if len(e)!=0:\n",
    "                tau=taugenerate(len(label),e,yt)\n",
    "                for k in label:\n",
    "                    m[k-1]=np.add(m[k-1],tau[k-1]*x)\n",
    "            #if i%1000==0:\n",
    "            if i%2==0:\n",
    "                y_bar1=[]       \n",
    "                for t in range(1,i+1):\n",
    "                    x=feature[t-1]\n",
    "                    ybart=np.argmax([safe_sparse_dot(m[j],x) for j in range(len(label))])+1\n",
    "                    y_bar1.append(ybart)\n",
    "                l=(1/i)*sum([np.absolute(y[j-1]-y_bar1[j-1]) for j in range(1,i+1)])\n",
    "                print(f\"loss in round {i} is \",l)\n",
    "                loss.append(l)\n",
    "    return loss,m         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "elder-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worstmulticlassalgo(feature,y,maxiter,d):\n",
    "    m=np.zeros((len(np.unique(y)),len(feature[0])))\n",
    "    loss=[]\n",
    "    label=range(1,len(np.unique(y))+1)\n",
    "    l2=np.sort(np.unique(y))\n",
    "    def taugenerate(h,m,x,yt,label):\n",
    "        t=[0 for i in range(h)]\n",
    "        t[int(yt-1)]=1\n",
    "        k=np.argsort([safe_sparse_dot(m[j],x) for j in range(h)])[-2]\n",
    "        t[k]=-1\n",
    "        return t\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(label)):\n",
    "            if y[i]==l2[j]:\n",
    "                y[i]=label[j]\n",
    "    for s in range(d):\n",
    "        for i in range(1,maxiter+1):\n",
    "            x=feature[i-1]\n",
    "            ybart=np.argmax([safe_sparse_dot(m[j],x) for j in range(len(label))])+1\n",
    "            yt=y[i-1]\n",
    "            e=[]\n",
    "            for r in range(1,len(label)+1):\n",
    "                if  r!=yt:\n",
    "                    if safe_sparse_dot(m[int(r-1)],x)>=safe_sparse_dot(m[int(yt-1)],x):\n",
    "                        e.append(r)\n",
    "            if len(e)!=0:\n",
    "                tau=taugenerate(len(label),m,x,yt,label)\n",
    "                for k in label:\n",
    "                    m[k-1]=np.add(m[k-1],tau[k-1]*x)\n",
    "            #if i%1000==0:\n",
    "            if i%2==0:\n",
    "                y_bar1=[]       \n",
    "                for t in range(1,i+1):\n",
    "                    x=feature[t-1]\n",
    "                    ybart=np.argmax([safe_sparse_dot(m[j],x) for j in range(len(label))])+1\n",
    "                    y_bar1.append(ybart)\n",
    "                l=(1/i)*sum([np.absolute(y[j-1]-y_bar1[j-1]) for j in range(1,i+1)])\n",
    "                print(f\"loss in round {i} is \",l)\n",
    "                loss.append(l)\n",
    "    return loss,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "deadly-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vimulticlassalgo(feature,y,maxiter,d):\n",
    "    m=np.zeros((len(np.unique(y)),len(feature[0])))\n",
    "    loss=[]\n",
    "    y_bar1=[]\n",
    "    label=range(1,len(np.unique(y))+1)\n",
    "    l2=np.sort(np.unique(y))\n",
    "    def taugenerate(h,m,x,yt):\n",
    "        t=[0 for i in range(h)]\n",
    "        t[int(yt-1)]=1\n",
    "        for i in range(h):\n",
    "            if i!=int(yt-1):\n",
    "                if safe_sparse_dot(m[i],x)-safe_sparse_dot(m[int(yt-1)],x)>0:\n",
    "                    num=safe_sparse_dot(m[i],x)-safe_sparse_dot(m[int(yt-1)],x)\n",
    "                else:\n",
    "                    num=0\n",
    "                den=0\n",
    "                for k in range(h):\n",
    "                    if safe_sparse_dot(m[k],x)-safe_sparse_dot(m[int(yt-1)],x)>0:\n",
    "                        den+=safe_sparse_dot(m[k],x)-safe_sparse_dot(m[int(yt-1)],x)\n",
    "                if den==0:\n",
    "                    den=1\n",
    "                t[i]=-(num/den)     \n",
    "        return t\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(label)):\n",
    "            if y[i]==l2[j]:\n",
    "                y[i]=label[j]\n",
    "    for s in range(d):\n",
    "        for i in range(1,maxiter+1):\n",
    "            x=feature[i-1]\n",
    "            ybart=np.argmax([safe_sparse_dot(m[j],x) for j in range(len(label))])+1\n",
    "            yt=y[i-1]\n",
    "            e=[]\n",
    "            for r in range(1,len(label)+1):\n",
    "                if  r!=yt:\n",
    "                    if safe_sparse_dot(m[r-1],x)>=safe_sparse_dot(m[int(yt-1)],x):\n",
    "                        e.append(r)\n",
    "            if len(e)!=0:\n",
    "                tau=taugenerate(len(label),m,x,yt)\n",
    "                for k in label:\n",
    "                    m[k-1]=np.add(m[k-1],tau[k-1]*x)\n",
    "            #if i%1000==0:\n",
    "            if i%2==0:\n",
    "                y_bar1=[]       \n",
    "                for t in range(1,i+1):\n",
    "                    x=feature[t-1]\n",
    "                    ybart=np.argmax([safe_sparse_dot(m[j],x) for j in range(len(label))])+1\n",
    "                    y_bar1.append(ybart)\n",
    "                l=(1/i)*sum([np.absolute(y[j-1]-y_bar1[j-1]) for j in range(1,i+1)])\n",
    "                print(f\"loss in round {i} is \",l)\n",
    "                loss.append(l)\n",
    "    return loss,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "nuclear-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mira(feature,y,maxiter,d):\n",
    "    m=np.ones((len(np.unique(y)),len(feature[0])))\n",
    "    y_bar1=[]\n",
    "    loss=[]\n",
    "    label=range(1,len(np.unique(y))+1)\n",
    "    l2=np.sort(np.unique(y))\n",
    "    def taugenerate(h,m,x,yt,ybart):\n",
    "        t=[0 for i in range(h)]\n",
    "        t[int(yt-1)]=1\n",
    "        tau=np.minimum(0.001, (((m[int(ybart-1)]-m[int(yt-1)])*x)+1.0)/(2.0*(safe_sparse_dot(x,x))))\n",
    "        t[int(yt-1)]=tau\n",
    "        t[int(ybart-1)]=-tau\n",
    "        return t\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(label)):\n",
    "            if y[i]==l2[j]:\n",
    "                y[i]=label[j]\n",
    "    for s in range(d):\n",
    "        for i in range(1,maxiter+1):\n",
    "            x=feature[i-1]\n",
    "            ybart=np.argmax([safe_sparse_dot(m[j],x) for j in range(len(label))])+1\n",
    "            yt=y[i-1]\n",
    "            tau=taugenerate(len(label),m,x,yt,ybart)\n",
    "            for k in label:\n",
    "                m[k-1]=np.add(m[k-1],tau[k-1]*x)\n",
    "            #if i%1000==0:\n",
    "            if i%2==0:\n",
    "                y_bar1=[]       \n",
    "                for t in range(1,i+1):\n",
    "                    x=feature[t-1]\n",
    "                    ybart=np.argmax([safe_sparse_dot(m[j],x) for j in range(len(label))])+1\n",
    "                    y_bar1.append(ybart)\n",
    "                l=(1/i)*sum([np.absolute(y[j-1]-y_bar1[j-1]) for j in range(1,i+1)])\n",
    "                print(f\"loss in round {i} is \",l)\n",
    "                loss.append(l)\n",
    "    return loss,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "polyphonic-bacon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in round 2 is  2.997972442541511\n",
      "loss in round 4 is  2.497834941111342\n",
      "loss in round 6 is  2.997177844755044\n",
      "loss in round 8 is  2.871500609992203\n",
      "loss in round 10 is  2.6964861221163647\n",
      "loss in round 12 is  2.6633010270263573\n",
      "loss in round 14 is  2.853306014805497\n",
      "loss in round 16 is  2.6835044122142215\n",
      "loss in round 18 is  2.829007408025245\n",
      "loss in round 20 is  2.6954612117129524\n",
      "loss in round 22 is  2.722780564758887\n",
      "loss in round 24 is  2.7034739473168283\n",
      "loss in round 26 is  2.6874196708415874\n",
      "loss in round 28 is  2.673459912172404\n",
      "loss in round 30 is  2.7614197027806884\n",
      "loss in round 32 is  2.713290797693657\n",
      "loss in round 34 is  2.7004470919275083\n",
      "loss in round 36 is  2.688735197523121\n",
      "loss in round 38 is  2.678232293800933\n",
      "loss in round 40 is  2.66895884875066\n",
      "loss in round 42 is  2.6366750353915114\n",
      "loss in round 44 is  2.6526360030948117\n",
      "loss in round 46 is  2.7106641124073483\n",
      "loss in round 48 is  2.7013331184834564\n",
      "loss in round 50 is  2.752819343868059\n",
      "loss in round 52 is  2.723338616210384\n",
      "loss in round 54 is  2.7698904672128917\n",
      "loss in round 56 is  2.741883930356446\n",
      "loss in round 58 is  2.7503344873031694\n",
      "loss in round 60 is  2.758247991082309\n",
      "loss in round 62 is  2.781673257909468\n",
      "loss in round 64 is  2.803682781799722\n",
      "loss in round 66 is  2.7486877369328933\n",
      "loss in round 68 is  2.741034005019982\n",
      "loss in round 70 is  2.719479160905902\n",
      "loss in round 72 is  2.713075741039066\n",
      "loss in round 74 is  2.720302404037443\n",
      "loss in round 76 is  2.727311329261242\n",
      "loss in round 78 is  2.733866345176984\n",
      "loss in round 80 is  2.740095506403826\n",
      "loss in round 82 is  2.7216855933947\n",
      "loss in round 84 is  2.716005227396096\n",
      "loss in round 86 is  2.687325083018659\n",
      "loss in round 88 is  2.6939906862582417\n",
      "loss in round 90 is  2.6671305830430585\n",
      "loss in round 92 is  2.66318577748234\n",
      "loss in round 94 is  2.691219175716912\n",
      "loss in round 96 is  2.6866973169884605\n",
      "loss in round 98 is  2.682412127833487\n",
      "loss in round 100 is  2.6684963266469497\n",
      "loss in round 102 is  2.635576586583055\n",
      "loss in round 104 is  2.6423563022364287\n",
      "loss in round 106 is  2.6298832830231564\n",
      "loss in round 108 is  2.5994745492039013\n",
      "loss in round 110 is  2.6154884858807024\n",
      "loss in round 112 is  2.6130602474997384\n",
      "loss in round 114 is  2.6194199899244306\n",
      "loss in round 116 is  2.6167850163185054\n",
      "loss in round 118 is  2.639840657304403\n",
      "loss in round 120 is  2.6204562416087716\n",
      "loss in round 122 is  2.609939223989569\n",
      "loss in round 124 is  2.599784194379614\n",
      "loss in round 126 is  2.6057776545297253\n",
      "loss in round 128 is  2.62704570714042\n",
      "loss in round 130 is  2.624620245282784\n",
      "loss in round 132 is  2.622363928402409\n",
      "loss in round 134 is  2.63505480246146\n",
      "loss in round 136 is  2.6472526287071876\n",
      "loss in round 138 is  2.659171757068412\n",
      "loss in round 140 is  2.6564613554459027\n",
      "loss in round 142 is  2.6609134058127357\n",
      "loss in round 144 is  2.658399148081811\n",
      "loss in round 146 is  2.655809215238373\n",
      "loss in round 148 is  2.6668496354064937\n",
      "loss in round 150 is  2.657516280380503\n",
      "loss in round 152 is  2.635340942305256\n",
      "loss in round 154 is  2.6267870339443706\n",
      "loss in round 156 is  2.631265479519964\n",
      "loss in round 158 is  2.616495189466113\n",
      "loss in round 160 is  2.5960568630995344\n",
      "loss in round 162 is  2.606811724126254\n",
      "loss in round 164 is  2.611291011751469\n",
      "loss in round 166 is  2.6096073908895914\n",
      "loss in round 168 is  2.5900882142461397\n",
      "loss in round 170 is  2.6003910854817893\n",
      "loss in round 172 is  2.587275169022902\n",
      "loss in round 174 is  2.5801057744503226\n",
      "loss in round 176 is  2.5845126912718506\n",
      "loss in round 178 is  2.600149456815926\n",
      "loss in round 180 is  2.598575887297904\n",
      "loss in round 182 is  2.5861580943296087\n",
      "loss in round 184 is  2.5849586376743696\n",
      "loss in round 186 is  2.5729259897173913\n",
      "loss in round 188 is  2.582510735712309\n",
      "loss in round 190 is  2.5918235124499818\n",
      "loss in round 192 is  2.5956686107058444\n",
      "loss in round 194 is  2.5890432753851913\n",
      "loss in round 196 is  2.5878968567658482\n",
      "loss in round 198 is  2.591792223799418\n",
      "loss in round 200 is  2.5904555348916993\n",
      "loss in round 202 is  2.594184182195189\n",
      "loss in round 204 is  2.59778742438628\n",
      "loss in round 206 is  2.5963431406343354\n",
      "loss in round 208 is  2.5855534681406978\n",
      "loss in round 210 is  2.5844792864262054\n",
      "loss in round 212 is  2.583338817495689\n",
      "loss in round 214 is  2.5821171823424107\n",
      "loss in round 216 is  2.5856567185908905\n",
      "loss in round 218 is  2.5845668041858048\n",
      "loss in round 220 is  2.5744297944758237\n",
      "loss in round 222 is  2.5823150750013433\n",
      "loss in round 224 is  2.581174627814968\n",
      "loss in round 226 is  2.593330972383573\n",
      "loss in round 228 is  2.592081309509504\n",
      "loss in round 230 is  2.5952107339196737\n",
      "loss in round 232 is  2.6067703205260733\n",
      "loss in round 234 is  2.613828202235726\n",
      "loss in round 236 is  2.6124484873886344\n",
      "loss in round 238 is  2.615349445383676\n",
      "loss in round 240 is  2.614115362844968\n",
      "loss in round 242 is  2.6046180187629444\n",
      "loss in round 244 is  2.607371196345056\n",
      "loss in round 246 is  2.6063072193620958\n",
      "loss in round 248 is  2.596925496131801\n",
      "loss in round 250 is  2.599834247701009\n",
      "loss in round 252 is  2.586839883024914\n",
      "loss in round 254 is  2.5936589664736736\n",
      "loss in round 256 is  2.59648308652769\n",
      "loss in round 258 is  2.5876951584438452\n",
      "loss in round 260 is  2.586781031616581\n",
      "loss in round 262 is  2.581967919415909\n",
      "loss in round 264 is  2.5923869876781893\n",
      "loss in round 266 is  2.5950316658995276\n",
      "loss in round 268 is  2.5977490863322874\n",
      "loss in round 270 is  2.592969251619662\n",
      "loss in round 272 is  2.592014170299114\n",
      "loss in round 274 is  2.5983113510432787\n",
      "loss in round 276 is  2.600808435264845\n",
      "loss in round 278 is  2.606865673457623\n",
      "loss in round 280 is  2.598576957841426\n",
      "loss in round 282 is  2.608013506498894\n",
      "loss in round 284 is  2.6033919976155446\n",
      "loss in round 286 is  2.612730014067885\n",
      "loss in round 288 is  2.601036597174834\n",
      "loss in round 290 is  2.6102744200258425\n",
      "loss in round 292 is  2.6124022556113236\n",
      "loss in round 294 is  2.6078870189726424\n",
      "loss in round 296 is  2.60351285211019\n",
      "loss in round 298 is  2.598999770244471\n",
      "loss in round 300 is  2.5947685308051933\n",
      "loss in round 302 is  2.590566287514306\n",
      "loss in round 304 is  2.5895225421890764\n",
      "loss in round 306 is  2.5854575651748313\n",
      "loss in round 308 is  2.5942734205862195\n",
      "loss in round 310 is  2.583671166602948\n",
      "loss in round 312 is  2.5923441877425555\n",
      "loss in round 314 is  2.5881942242320526\n",
      "loss in round 316 is  2.59350567172293\n",
      "loss in round 318 is  2.598715965480852\n",
      "loss in round 320 is  2.600896467971796\n",
      "loss in round 322 is  2.5967489617645665\n",
      "loss in round 324 is  2.6019524232643625\n",
      "loss in round 326 is  2.6070843935767103\n",
      "loss in round 328 is  2.61514779505006\n",
      "loss in round 330 is  2.605096379561805\n",
      "loss in round 332 is  2.604199435902218\n",
      "loss in round 334 is  2.606100629506136\n",
      "loss in round 336 is  2.60507934637646\n",
      "loss in round 338 is  2.609988244189609\n",
      "loss in round 340 is  2.6030176525909217\n",
      "loss in round 342 is  2.605074244976442\n",
      "loss in round 344 is  2.598416951446072\n",
      "loss in round 346 is  2.591730862966508\n",
      "loss in round 348 is  2.582286427518063\n",
      "loss in round 350 is  2.5844262238980975\n",
      "loss in round 352 is  2.577914523685927\n",
      "loss in round 354 is  2.5716099442275886\n",
      "loss in round 356 is  2.579210111987848\n",
      "loss in round 358 is  2.5838760803351235\n",
      "loss in round 360 is  2.5830302506035734\n",
      "loss in round 362 is  2.5850380049266777\n",
      "loss in round 364 is  2.584178941083568\n",
      "loss in round 366 is  2.583293165240098\n",
      "loss in round 368 is  2.5877910156696937\n",
      "loss in round 370 is  2.5922457280168607\n",
      "loss in round 372 is  2.594093202189514\n",
      "loss in round 374 is  2.590659459426856\n",
      "loss in round 376 is  2.587185730823069\n",
      "loss in round 378 is  2.5784561074096586\n",
      "loss in round 380 is  2.575184286201281\n",
      "loss in round 382 is  2.5769731513655376\n",
      "loss in round 384 is  2.5685414572176732\n",
      "loss in round 386 is  2.567912206571001\n",
      "loss in round 388 is  2.5722574716624336\n",
      "loss in round 390 is  2.5714904046969047\n",
      "loss in round 392 is  2.568112992788727\n",
      "loss in round 394 is  2.5751071862863117\n",
      "loss in round 396 is  2.5692218570652865\n",
      "loss in round 398 is  2.5685518361410464\n",
      "loss in round 400 is  2.575188227257393\n",
      "loss in round 402 is  2.5769503473669193\n",
      "loss in round 404 is  2.5811377404232205\n",
      "loss in round 406 is  2.5828401652938995\n",
      "loss in round 408 is  2.584534310055082\n",
      "loss in round 410 is  2.5835984849691687\n",
      "loss in round 412 is  2.578140286578569\n",
      "loss in round 414 is  2.5726663726565557\n",
      "loss in round 416 is  2.5721645609226655\n",
      "loss in round 418 is  2.5762902370519316\n",
      "loss in round 420 is  2.573273015567464\n",
      "loss in round 422 is  2.5702343136772066\n",
      "loss in round 424 is  2.56948814175133\n",
      "loss in round 426 is  2.5617950304944834\n",
      "loss in round 428 is  2.5588929503381967\n",
      "loss in round 430 is  2.5605815265237095\n",
      "loss in round 432 is  2.555458504976216\n",
      "loss in round 434 is  2.5503770320091643\n",
      "loss in round 436 is  2.5430676942068478\n",
      "loss in round 438 is  2.5448767754985258\n",
      "loss in round 440 is  2.5465685902026034\n",
      "loss in round 442 is  2.5504723114806933\n",
      "loss in round 444 is  2.5565578904225337\n",
      "loss in round 446 is  2.5582765759144186\n",
      "loss in round 448 is  2.557637534254223\n",
      "loss in round 450 is  2.5636954556633094\n",
      "loss in round 452 is  2.562980272309548\n",
      "loss in round 454 is  2.568985872427055\n",
      "loss in round 456 is  2.5682274127489735\n",
      "loss in round 458 is  2.5654056049941305\n",
      "loss in round 460 is  2.5647324224026886\n",
      "loss in round 462 is  2.570376543298442\n",
      "loss in round 464 is  2.5740280306971273\n",
      "loss in round 466 is  2.5773629921443475\n",
      "loss in round 468 is  2.5767004600078014\n",
      "loss in round 470 is  2.582239524666057\n",
      "loss in round 472 is  2.5876081148272156\n",
      "loss in round 474 is  2.5847426558069184\n",
      "loss in round 476 is  2.586165711408478\n",
      "loss in round 478 is  2.5813394577010613\n",
      "loss in round 480 is  2.586897516295227\n",
      "loss in round 482 is  2.5841835776591906\n",
      "loss in round 484 is  2.5834581161207173\n",
      "loss in round 486 is  2.58078978574306\n",
      "loss in round 488 is  2.586273532103287\n",
      "loss in round 490 is  2.5916333289208247\n",
      "loss in round 492 is  2.593008225541835\n",
      "loss in round 494 is  2.5921896223520546\n",
      "loss in round 496 is  2.5875553199581405\n",
      "loss in round 498 is  2.5808941229929276\n",
      "loss in round 500 is  2.586306185439435\n",
      "loss in round 502 is  2.587319926035633\n",
      "loss in round 504 is  2.5905006757225313\n",
      "loss in round 506 is  2.589670218063275\n",
      "loss in round 508 is  2.586997034674126\n",
      "loss in round 510 is  2.590145340371877\n",
      "loss in round 512 is  2.5912898877007926\n",
      "loss in round 514 is  2.5942948900619487\n",
      "loss in round 516 is  2.5933289832709967\n",
      "loss in round 518 is  2.5927038372414226\n",
      "loss in round 520 is  2.5977164705535913\n",
      "loss in round 522 is  2.5968901320580553\n",
      "loss in round 524 is  2.599924350727843\n",
      "loss in round 526 is  2.6047290481749568\n",
      "loss in round 528 is  2.605794539906814\n",
      "loss in round 530 is  2.606751203788062\n",
      "loss in round 532 is  2.6060688218689254\n",
      "loss in round 534 is  2.6016768400095716\n",
      "loss in round 536 is  2.600686878863239\n",
      "loss in round 538 is  2.5981426949206097\n",
      "loss in round 540 is  2.5991765810737957\n",
      "loss in round 542 is  2.594870965932583\n",
      "loss in round 544 is  2.592438042563174\n",
      "loss in round 546 is  2.593607698042906\n",
      "loss in round 548 is  2.5965340620245803\n",
      "loss in round 550 is  2.5976291528167152\n",
      "loss in round 552 is  2.6006317188950243\n",
      "loss in round 554 is  2.5980070862240474\n",
      "loss in round 556 is  2.5972630787124475\n",
      "loss in round 558 is  2.5999927408126267\n",
      "loss in round 560 is  2.597499961023788\n",
      "loss in round 562 is  2.5934007423236043\n",
      "loss in round 564 is  2.5979603599121086\n",
      "loss in round 566 is  2.5971570100330323\n",
      "loss in round 568 is  2.598228413198551\n",
      "loss in round 570 is  2.5940582890031365\n",
      "loss in round 572 is  2.5985945034314284\n",
      "loss in round 574 is  2.6029899734701307\n",
      "loss in round 576 is  2.602200756668913\n",
      "loss in round 578 is  2.598026240354104\n",
      "loss in round 580 is  2.6008252014288504\n",
      "loss in round 582 is  2.5984480760632778\n",
      "loss in round 584 is  2.5993242258629636\n",
      "loss in round 586 is  2.600315459457933\n",
      "loss in round 588 is  2.601401018225728\n",
      "loss in round 590 is  2.5973954540614477\n",
      "loss in round 592 is  2.6017716945994427\n",
      "loss in round 594 is  2.6059763873016704\n",
      "loss in round 596 is  2.6068916545892185\n",
      "loss in round 598 is  2.6079003002871297\n",
      "loss in round 600 is  2.607181999432024\n",
      "loss in round 602 is  2.6114472077889395\n",
      "loss in round 604 is  2.6106575720759806\n",
      "loss in round 606 is  2.610036022883241\n",
      "loss in round 608 is  2.609150460746839\n",
      "loss in round 610 is  2.6102303149696873\n",
      "loss in round 612 is  2.6062310427331057\n",
      "loss in round 614 is  2.6104227570124143\n",
      "loss in round 616 is  2.614460476831152\n",
      "loss in round 618 is  2.612054184483125\n",
      "loss in round 620 is  2.6082605815941107\n",
      "loss in round 622 is  2.604226631973593\n",
      "loss in round 624 is  2.6036108047538935\n",
      "loss in round 626 is  2.6046359093120843\n",
      "loss in round 628 is  2.602192717631082\n",
      "loss in round 630 is  2.6015912491832816\n",
      "loss in round 632 is  2.6038602770591592\n",
      "loss in round 634 is  2.5999732512934925\n",
      "loss in round 636 is  2.6023914553718344\n",
      "loss in round 638 is  2.6015486003089676\n",
      "loss in round 640 is  2.5994794390911924\n",
      "loss in round 642 is  2.598801100944753\n",
      "loss in round 644 is  2.5997896573078525\n",
      "loss in round 646 is  2.602139261662253\n",
      "loss in round 648 is  2.6043515630378455\n",
      "loss in round 650 is  2.60523277526876\n",
      "loss in round 652 is  2.604484014892444\n",
      "loss in round 654 is  2.6068975237455403\n",
      "loss in round 656 is  2.6046189458924207\n",
      "loss in round 658 is  2.6025154855943047\n",
      "loss in round 660 is  2.603269442024918\n",
      "loss in round 662 is  2.602595364553948\n",
      "loss in round 664 is  2.5989940277664694\n",
      "loss in round 666 is  2.5953743356088426\n",
      "loss in round 668 is  2.593400309176869\n",
      "loss in round 670 is  2.595815506896533\n",
      "loss in round 672 is  2.5965784233694325\n",
      "loss in round 674 is  2.598927874477977\n",
      "loss in round 676 is  2.5952386243666252\n",
      "loss in round 678 is  2.596068295095264\n",
      "loss in round 680 is  2.596861423259282\n",
      "loss in round 682 is  2.5962439327044593\n",
      "loss in round 684 is  2.597224872785722\n",
      "loss in round 686 is  2.5936323911285624\n",
      "loss in round 688 is  2.5957732799468314\n",
      "loss in round 690 is  2.5950954720258177\n",
      "loss in round 692 is  2.597276911970062\n",
      "loss in round 694 is  2.597939913590851\n",
      "loss in round 696 is  2.5941700948460897\n",
      "loss in round 698 is  2.5907707193839293\n",
      "loss in round 700 is  2.585883294420566\n",
      "loss in round 702 is  2.5825694273344664\n",
      "loss in round 704 is  2.5849298382200647\n",
      "loss in round 706 is  2.585746631100942\n",
      "loss in round 708 is  2.582339338868844\n",
      "loss in round 710 is  2.581767370577671\n",
      "loss in round 712 is  2.5826026919461955\n",
      "loss in round 714 is  2.5846452618900524\n",
      "loss in round 716 is  2.5825906468265662\n",
      "loss in round 718 is  2.5848098491943\n",
      "loss in round 720 is  2.58568537386155\n",
      "loss in round 722 is  2.5851039720213955\n",
      "loss in round 724 is  2.5830164705323253\n",
      "loss in round 726 is  2.583920580530565\n",
      "loss in round 728 is  2.5819724255057848\n",
      "loss in round 730 is  2.581528618247559\n",
      "loss in round 732 is  2.582225047105712\n",
      "loss in round 734 is  2.5816243277397684\n",
      "loss in round 736 is  2.5770501873214107\n",
      "loss in round 738 is  2.5751566290997507\n",
      "loss in round 740 is  2.5773528232840217\n",
      "loss in round 742 is  2.580800561510369\n",
      "loss in round 744 is  2.580109067186655\n",
      "loss in round 746 is  2.575638117540205\n",
      "loss in round 748 is  2.572471432222718\n",
      "loss in round 750 is  2.575926303250388\n",
      "loss in round 752 is  2.5741121705141774\n",
      "loss in round 754 is  2.5762345051640954\n",
      "loss in round 756 is  2.579646778204476\n",
      "loss in round 758 is  2.5803351110797528\n",
      "loss in round 760 is  2.5784476430256555\n",
      "loss in round 762 is  2.580433730387644\n",
      "loss in round 764 is  2.579841216501778\n",
      "loss in round 766 is  2.5805629720996803\n",
      "loss in round 768 is  2.5838662046027068\n",
      "loss in round 770 is  2.5870756533174095\n",
      "loss in round 772 is  2.587826665751107\n",
      "loss in round 774 is  2.5897144461474584\n",
      "loss in round 776 is  2.587900017981743\n",
      "loss in round 778 is  2.5873212492901203\n",
      "loss in round 780 is  2.5880129620582126\n",
      "loss in round 782 is  2.5885320876667843\n",
      "loss in round 784 is  2.586751311393386\n",
      "loss in round 786 is  2.584889453241942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-dceb1ba2bf5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidrowhoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-53d2e3d67ebb>\u001b[0m in \u001b[0;36mwidrowhoff\u001b[0;34m(train, y, t, c, m)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     \u001b[0my_bart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                     \u001b[0my_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_bart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_bar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss1,w1=widrowhoff(x,y,len(y),0.001,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/22n0457/imdb/loss1.csv',loss1, delimiter=',')\n",
    "np.savetxt('/home/22n0457/imdb/w1.csv',w1, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "similar-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in round 2 is  1.0\n",
      "loss in round 4 is  0.5\n",
      "loss in round 6 is  0.6666666666666666\n",
      "loss in round 8 is  0.625\n",
      "loss in round 10 is  1.1\n",
      "loss in round 12 is  0.8333333333333334\n",
      "loss in round 14 is  0.7142857142857143\n",
      "loss in round 16 is  0.875\n",
      "loss in round 18 is  0.5555555555555556\n",
      "loss in round 20 is  0.8\n",
      "loss in round 22 is  0.6363636363636364\n",
      "loss in round 24 is  0.7083333333333334\n",
      "loss in round 26 is  0.6538461538461539\n",
      "loss in round 28 is  0.6071428571428571\n",
      "loss in round 30 is  0.6666666666666666\n",
      "loss in round 32 is  0.71875\n",
      "loss in round 34 is  0.5294117647058824\n",
      "loss in round 36 is  0.6666666666666666\n",
      "loss in round 38 is  0.6052631578947368\n",
      "loss in round 40 is  0.675\n",
      "loss in round 42 is  0.6666666666666666\n",
      "loss in round 44 is  0.6363636363636364\n",
      "loss in round 46 is  0.717391304347826\n",
      "loss in round 48 is  0.5625\n",
      "loss in round 50 is  0.58\n",
      "loss in round 52 is  0.5384615384615384\n",
      "loss in round 54 is  0.8148148148148148\n",
      "loss in round 56 is  0.625\n",
      "loss in round 58 is  0.6379310344827587\n",
      "loss in round 60 is  0.7166666666666667\n",
      "loss in round 62 is  0.7096774193548387\n",
      "loss in round 64 is  0.703125\n",
      "loss in round 66 is  0.7424242424242424\n",
      "loss in round 68 is  0.7794117647058824\n",
      "loss in round 70 is  0.9428571428571428\n",
      "loss in round 72 is  0.8194444444444444\n",
      "loss in round 74 is  0.7837837837837838\n",
      "loss in round 76 is  0.6710526315789473\n",
      "loss in round 78 is  0.6666666666666666\n",
      "loss in round 80 is  0.6125\n",
      "loss in round 82 is  0.5975609756097561\n",
      "loss in round 84 is  0.5952380952380952\n",
      "loss in round 86 is  0.7325581395348837\n",
      "loss in round 88 is  0.7159090909090909\n",
      "loss in round 90 is  0.7\n",
      "loss in round 92 is  0.7717391304347826\n",
      "loss in round 94 is  0.9042553191489362\n",
      "loss in round 96 is  0.5833333333333334\n",
      "loss in round 98 is  0.7448979591836735\n",
      "loss in round 100 is  0.7\n",
      "loss in round 102 is  0.6862745098039216\n",
      "loss in round 104 is  0.6826923076923077\n",
      "loss in round 106 is  0.6132075471698113\n",
      "loss in round 108 is  0.6851851851851852\n",
      "loss in round 110 is  0.6181818181818182\n",
      "loss in round 112 is  0.75\n",
      "loss in round 114 is  0.7105263157894737\n",
      "loss in round 116 is  0.6120689655172413\n",
      "loss in round 118 is  0.6694915254237288\n",
      "loss in round 120 is  0.7333333333333333\n",
      "loss in round 122 is  0.7213114754098361\n",
      "loss in round 124 is  0.7016129032258065\n",
      "loss in round 126 is  0.7142857142857143\n",
      "loss in round 128 is  0.7421875\n",
      "loss in round 130 is  0.7076923076923077\n",
      "loss in round 132 is  0.6590909090909091\n",
      "loss in round 134 is  0.7089552238805971\n",
      "loss in round 136 is  0.7132352941176471\n",
      "loss in round 138 is  0.6594202898550725\n",
      "loss in round 140 is  0.7214285714285714\n",
      "loss in round 142 is  0.6267605633802817\n",
      "loss in round 144 is  0.7222222222222222\n",
      "loss in round 146 is  0.9657534246575342\n",
      "loss in round 148 is  0.7094594594594594\n",
      "loss in round 150 is  0.7066666666666667\n",
      "loss in round 152 is  0.9671052631578947\n",
      "loss in round 154 is  0.6233766233766234\n",
      "loss in round 156 is  0.6346153846153846\n",
      "loss in round 158 is  0.689873417721519\n",
      "loss in round 160 is  0.68125\n",
      "loss in round 162 is  0.6296296296296297\n",
      "loss in round 164 is  0.6646341463414634\n",
      "loss in round 166 is  0.6566265060240963\n",
      "loss in round 168 is  0.9404761904761905\n",
      "loss in round 170 is  0.6294117647058823\n",
      "loss in round 172 is  0.6802325581395349\n",
      "loss in round 174 is  0.9310344827586207\n",
      "loss in round 176 is  0.5965909090909091\n",
      "loss in round 178 is  0.9438202247191011\n",
      "loss in round 180 is  0.6555555555555556\n",
      "loss in round 182 is  0.6428571428571429\n",
      "loss in round 184 is  0.6521739130434783\n",
      "loss in round 186 is  0.7096774193548387\n",
      "loss in round 188 is  0.7340425531914894\n",
      "loss in round 190 is  0.7157894736842105\n",
      "loss in round 192 is  0.6666666666666666\n",
      "loss in round 194 is  0.7061855670103093\n",
      "loss in round 196 is  0.673469387755102\n",
      "loss in round 198 is  0.7222222222222222\n",
      "loss in round 200 is  0.725\n",
      "loss in round 202 is  0.6485148514851485\n",
      "loss in round 204 is  0.6519607843137255\n",
      "loss in round 206 is  0.7184466019417476\n",
      "loss in round 208 is  0.7211538461538461\n",
      "loss in round 210 is  0.6619047619047619\n",
      "loss in round 212 is  0.6132075471698113\n",
      "loss in round 214 is  0.7149532710280374\n",
      "loss in round 216 is  0.6574074074074074\n",
      "loss in round 218 is  0.6009174311926605\n",
      "loss in round 220 is  0.7136363636363636\n",
      "loss in round 222 is  0.7117117117117117\n",
      "loss in round 224 is  0.7098214285714286\n",
      "loss in round 226 is  0.9203539823008849\n",
      "loss in round 228 is  0.6973684210526315\n",
      "loss in round 230 is  0.6956521739130435\n",
      "loss in round 232 is  0.6896551724137931\n",
      "loss in round 234 is  0.7222222222222222\n",
      "loss in round 236 is  0.5932203389830508\n",
      "loss in round 238 is  0.6386554621848739\n",
      "loss in round 240 is  0.7\n",
      "loss in round 242 is  0.731404958677686\n",
      "loss in round 244 is  0.6352459016393442\n",
      "loss in round 246 is  0.7398373983739838\n",
      "loss in round 248 is  0.9637096774193549\n",
      "loss in round 250 is  0.74\n",
      "loss in round 252 is  0.9523809523809523\n",
      "loss in round 254 is  0.7047244094488189\n",
      "loss in round 256 is  0.640625\n",
      "loss in round 258 is  0.7325581395348837\n",
      "loss in round 260 is  0.6192307692307693\n",
      "loss in round 262 is  0.648854961832061\n",
      "loss in round 264 is  0.6136363636363636\n",
      "loss in round 266 is  0.650375939849624\n",
      "loss in round 268 is  0.6194029850746269\n",
      "loss in round 270 is  0.6148148148148148\n",
      "loss in round 272 is  0.6911764705882353\n",
      "loss in round 274 is  0.6934306569343066\n",
      "loss in round 276 is  0.6920289855072463\n",
      "loss in round 278 is  0.6906474820143885\n",
      "loss in round 280 is  0.5928571428571429\n",
      "loss in round 282 is  0.6808510638297872\n",
      "loss in round 284 is  0.6373239436619719\n",
      "loss in round 286 is  0.6083916083916084\n",
      "loss in round 288 is  0.7291666666666666\n",
      "loss in round 290 is  0.6827586206896552\n",
      "loss in round 292 is  0.7226027397260274\n",
      "loss in round 294 is  0.9115646258503401\n",
      "loss in round 296 is  0.6621621621621622\n",
      "loss in round 298 is  0.6577181208053692\n",
      "loss in round 300 is  0.63\n",
      "loss in round 302 is  0.5596026490066225\n",
      "loss in round 304 is  0.6842105263157895\n",
      "loss in round 306 is  0.6274509803921569\n",
      "loss in round 308 is  0.6071428571428571\n",
      "loss in round 310 is  0.9096774193548387\n",
      "loss in round 312 is  0.6858974358974359\n",
      "loss in round 314 is  0.643312101910828\n",
      "loss in round 316 is  0.6487341772151899\n",
      "loss in round 318 is  0.6666666666666666\n",
      "loss in round 320 is  0.5625\n",
      "loss in round 322 is  0.6211180124223602\n",
      "loss in round 324 is  0.6666666666666666\n",
      "loss in round 326 is  0.6625766871165644\n",
      "loss in round 328 is  0.6585365853658537\n",
      "loss in round 330 is  0.6484848484848484\n",
      "loss in round 332 is  0.5512048192771084\n",
      "loss in round 334 is  0.625748502994012\n",
      "loss in round 336 is  0.6934523809523809\n",
      "loss in round 338 is  0.6449704142011834\n",
      "loss in round 340 is  0.638235294117647\n",
      "loss in round 342 is  0.6842105263157895\n",
      "loss in round 344 is  0.6802325581395349\n",
      "loss in round 346 is  0.9364161849710982\n",
      "loss in round 348 is  1.1494252873563218\n",
      "loss in round 350 is  0.72\n",
      "loss in round 352 is  0.7159090909090909\n",
      "loss in round 354 is  0.711864406779661\n",
      "loss in round 356 is  0.6741573033707865\n",
      "loss in round 358 is  0.6759776536312849\n",
      "loss in round 360 is  0.6666666666666666\n",
      "loss in round 362 is  0.5939226519337016\n",
      "loss in round 364 is  0.6593406593406593\n",
      "loss in round 366 is  0.6557377049180327\n",
      "loss in round 368 is  0.6385869565217391\n",
      "loss in round 370 is  0.6432432432432432\n",
      "loss in round 372 is  0.5806451612903226\n",
      "loss in round 374 is  0.6443850267379679\n",
      "loss in round 376 is  0.6409574468085106\n",
      "loss in round 378 is  0.9206349206349206\n",
      "loss in round 380 is  0.6368421052631579\n",
      "loss in round 382 is  0.7277486910994765\n",
      "loss in round 384 is  0.7239583333333334\n",
      "loss in round 386 is  0.6683937823834197\n",
      "loss in round 388 is  0.6726804123711341\n",
      "loss in round 390 is  0.5923076923076923\n",
      "loss in round 392 is  0.7372448979591837\n",
      "loss in round 394 is  0.6040609137055838\n",
      "loss in round 396 is  0.6590909090909091\n",
      "loss in round 398 is  0.8844221105527639\n",
      "loss in round 400 is  0.88\n",
      "loss in round 402 is  0.6865671641791045\n",
      "loss in round 404 is  0.6905940594059405\n",
      "loss in round 406 is  0.603448275862069\n",
      "loss in round 408 is  0.5931372549019608\n",
      "loss in round 410 is  0.7585365853658537\n",
      "loss in round 412 is  0.6966019417475728\n",
      "loss in round 414 is  0.7536231884057971\n",
      "loss in round 416 is  0.6658653846153846\n",
      "loss in round 418 is  0.7081339712918661\n",
      "loss in round 420 is  0.611904761904762\n",
      "loss in round 422 is  0.6777251184834123\n",
      "loss in round 424 is  0.6108490566037735\n",
      "loss in round 426 is  0.6596244131455399\n",
      "loss in round 428 is  0.6565420560747663\n",
      "loss in round 430 is  0.6162790697674418\n",
      "loss in round 432 is  0.7453703703703703\n",
      "loss in round 434 is  0.7419354838709677\n",
      "loss in round 436 is  0.7385321100917431\n",
      "loss in round 438 is  0.6164383561643836\n",
      "loss in round 440 is  0.65\n",
      "loss in round 442 is  0.7330316742081447\n",
      "loss in round 444 is  0.9301801801801802\n",
      "loss in round 446 is  0.6345291479820628\n",
      "loss in round 448 is  0.6160714285714286\n",
      "loss in round 450 is  0.6444444444444445\n",
      "loss in round 452 is  0.6194690265486725\n",
      "loss in round 454 is  0.7290748898678414\n",
      "loss in round 456 is  0.6842105263157895\n",
      "loss in round 458 is  0.6157205240174672\n",
      "loss in round 460 is  0.6195652173913043\n",
      "loss in round 462 is  0.9372294372294372\n",
      "loss in round 464 is  0.9396551724137931\n",
      "loss in round 466 is  0.7188841201716738\n",
      "loss in round 468 is  0.7286324786324786\n",
      "loss in round 470 is  0.725531914893617\n",
      "loss in round 472 is  0.722457627118644\n",
      "loss in round 474 is  0.70042194092827\n",
      "loss in round 476 is  0.6050420168067226\n",
      "loss in round 478 is  0.7928870292887029\n",
      "loss in round 480 is  0.6354166666666666\n",
      "loss in round 482 is  0.6327800829875518\n",
      "loss in round 484 is  0.6983471074380165\n",
      "loss in round 486 is  0.6337448559670782\n",
      "loss in round 488 is  0.9077868852459017\n",
      "loss in round 490 is  0.9040816326530612\n",
      "loss in round 492 is  0.6422764227642277\n",
      "loss in round 494 is  0.819838056680162\n",
      "loss in round 496 is  0.8165322580645161\n",
      "loss in round 498 is  1.0160642570281124\n",
      "loss in round 500 is  0.924\n",
      "loss in round 502 is  0.7051792828685259\n",
      "loss in round 504 is  0.6984126984126984\n",
      "loss in round 506 is  0.6600790513833992\n",
      "loss in round 508 is  0.6850393700787402\n",
      "loss in round 510 is  0.592156862745098\n",
      "loss in round 512 is  0.583984375\n",
      "loss in round 514 is  0.642023346303502\n",
      "loss in round 516 is  0.6434108527131783\n",
      "loss in round 518 is  0.696911196911197\n",
      "loss in round 520 is  0.8826923076923077\n",
      "loss in round 522 is  0.6551724137931034\n",
      "loss in round 524 is  0.7041984732824428\n",
      "loss in round 526 is  0.6920152091254753\n",
      "loss in round 528 is  0.8768939393939394\n",
      "loss in round 530 is  0.6113207547169811\n",
      "loss in round 532 is  0.7161654135338346\n",
      "loss in round 534 is  0.9026217228464419\n",
      "loss in round 536 is  0.8414179104477612\n",
      "loss in round 538 is  0.9572490706319703\n",
      "loss in round 540 is  0.6370370370370371\n",
      "loss in round 542 is  0.8302583025830258\n",
      "loss in round 544 is  0.6452205882352942\n",
      "loss in round 546 is  0.6428571428571429\n",
      "loss in round 548 is  0.6934306569343066\n",
      "loss in round 550 is  0.6327272727272727\n",
      "loss in round 552 is  0.8876811594202898\n",
      "loss in round 554 is  0.6931407942238267\n",
      "loss in round 556 is  0.7194244604316546\n",
      "loss in round 558 is  0.6971326164874552\n",
      "loss in round 560 is  0.5553571428571429\n",
      "loss in round 562 is  0.6281138790035588\n",
      "loss in round 564 is  0.625886524822695\n",
      "loss in round 566 is  0.6219081272084805\n",
      "loss in round 568 is  0.6197183098591549\n",
      "loss in round 570 is  0.6175438596491228\n",
      "loss in round 572 is  0.6153846153846154\n",
      "loss in round 574 is  0.8815331010452961\n",
      "loss in round 576 is  0.6944444444444444\n",
      "loss in round 578 is  0.8910034602076125\n",
      "loss in round 580 is  0.6862068965517242\n",
      "loss in round 582 is  0.6597938144329897\n",
      "loss in round 584 is  0.5958904109589042\n",
      "loss in round 586 is  0.5938566552901023\n",
      "loss in round 588 is  0.5935374149659864\n",
      "loss in round 590 is  0.9050847457627119\n",
      "loss in round 592 is  0.7010135135135135\n",
      "loss in round 594 is  0.6986531986531986\n",
      "loss in round 596 is  0.6593959731543624\n",
      "loss in round 598 is  0.5936454849498328\n",
      "loss in round 600 is  0.705\n",
      "loss in round 602 is  0.8388704318936877\n",
      "loss in round 604 is  0.7135761589403974\n",
      "loss in round 606 is  0.6914191419141914\n",
      "loss in round 608 is  0.6891447368421053\n",
      "loss in round 610 is  0.6622950819672131\n",
      "loss in round 612 is  0.684640522875817\n",
      "loss in round 614 is  0.6824104234527687\n",
      "loss in round 616 is  0.836038961038961\n",
      "loss in round 618 is  0.5776699029126213\n",
      "loss in round 620 is  0.7661290322580645\n",
      "loss in round 622 is  0.8794212218649518\n",
      "loss in round 624 is  0.6891025641025641\n",
      "loss in round 626 is  0.6741214057507987\n",
      "loss in round 628 is  0.6735668789808917\n",
      "loss in round 630 is  0.8777777777777778\n",
      "loss in round 632 is  0.685126582278481\n",
      "loss in round 634 is  0.8470031545741324\n",
      "loss in round 636 is  0.6965408805031447\n",
      "loss in round 638 is  0.6489028213166145\n",
      "loss in round 640 is  0.6796875\n",
      "loss in round 642 is  0.8738317757009346\n",
      "loss in round 644 is  0.6956521739130435\n",
      "loss in round 646 is  0.6795665634674922\n",
      "loss in round 648 is  0.683641975308642\n",
      "loss in round 650 is  0.5876923076923077\n",
      "loss in round 652 is  0.6503067484662577\n",
      "loss in round 654 is  0.6865443425076453\n",
      "loss in round 656 is  0.6920731707317073\n",
      "loss in round 658 is  0.6899696048632219\n",
      "loss in round 660 is  0.5742424242424242\n",
      "loss in round 662 is  0.6102719033232629\n",
      "loss in round 664 is  0.6069277108433735\n",
      "loss in round 666 is  0.7252252252252253\n",
      "loss in round 668 is  0.6631736526946108\n",
      "loss in round 670 is  0.6313432835820896\n",
      "loss in round 672 is  0.8824404761904762\n",
      "loss in round 674 is  0.6513353115727003\n",
      "loss in round 676 is  0.5931952662721893\n",
      "loss in round 678 is  0.7271386430678466\n",
      "loss in round 680 is  0.7191176470588235\n",
      "loss in round 682 is  0.593841642228739\n",
      "loss in round 684 is  0.5921052631578947\n",
      "loss in round 686 is  0.9008746355685131\n",
      "loss in round 688 is  0.5988372093023255\n",
      "loss in round 690 is  0.8130434782608695\n",
      "loss in round 692 is  0.6791907514450867\n",
      "loss in round 694 is  0.6959654178674352\n",
      "loss in round 696 is  0.8563218390804598\n",
      "loss in round 698 is  0.8538681948424068\n",
      "loss in round 700 is  0.8514285714285714\n",
      "loss in round 702 is  0.7507122507122507\n",
      "loss in round 704 is  0.5653409090909091\n",
      "loss in round 706 is  0.7577903682719547\n",
      "loss in round 708 is  0.8700564971751412\n",
      "loss in round 710 is  0.6154929577464788\n",
      "loss in round 712 is  0.5730337078651685\n",
      "loss in round 714 is  0.6750700280112045\n",
      "loss in round 716 is  0.5768156424581006\n",
      "loss in round 718 is  0.6142061281337048\n",
      "loss in round 720 is  0.6736111111111112\n",
      "loss in round 722 is  0.8933518005540166\n",
      "loss in round 724 is  0.7513812154696132\n",
      "loss in round 726 is  0.5826446280991735\n",
      "loss in round 728 is  0.75\n",
      "loss in round 730 is  0.8575342465753425\n",
      "loss in round 732 is  0.7349726775956285\n",
      "loss in round 734 is  0.5626702997275205\n",
      "loss in round 736 is  0.8315217391304348\n",
      "loss in round 738 is  0.8455284552845529\n",
      "loss in round 740 is  0.6810810810810811\n",
      "loss in round 742 is  0.6792452830188679\n",
      "loss in round 744 is  0.6774193548387096\n",
      "loss in round 746 is  0.675603217158177\n",
      "loss in round 748 is  0.8516042780748663\n",
      "loss in round 750 is  0.5786666666666667\n",
      "loss in round 752 is  0.6595744680851063\n",
      "loss in round 754 is  0.9960212201591512\n",
      "loss in round 756 is  0.9933862433862434\n",
      "loss in round 758 is  0.7664907651715039\n",
      "loss in round 760 is  0.6131578947368421\n",
      "loss in round 762 is  0.6115485564304461\n",
      "loss in round 764 is  0.6335078534031413\n",
      "loss in round 766 is  0.5300261096605744\n",
      "loss in round 768 is  0.5286458333333334\n",
      "loss in round 770 is  0.8181818181818182\n",
      "loss in round 772 is  0.8290155440414507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-d78a55e7a42a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpranking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-60a95a09d065>\u001b[0m in \u001b[0;36mpranking\u001b[0;34m(train, y, t, m)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m#if i%1000==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0my_bar1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_bar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss in round {i} is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-60a95a09d065>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m#if i%1000==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0my_bar1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_bar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss in round {i} is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-60a95a09d065>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_y\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss2,w2,b2=pranking(x,y,len(y),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/22n0457/imdb/loss2.csv',loss2, delimiter=',')\n",
    "np.savetxt('/home/22n0457/imdb/w2.csv',w2, delimiter=',')\n",
    "np.savetxt('/home/22n0457/imdb/b2.csv',b2, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "liberal-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in round 2 is  0.0\n",
      "loss in round 4 is  0.0\n",
      "loss in round 6 is  0.0\n",
      "loss in round 8 is  0.5\n",
      "loss in round 10 is  0.5\n",
      "loss in round 12 is  0.5833333333333333\n",
      "loss in round 14 is  0.5\n",
      "loss in round 16 is  0.25\n",
      "loss in round 18 is  0.2222222222222222\n",
      "loss in round 20 is  0.2\n",
      "loss in round 22 is  0.18181818181818182\n",
      "loss in round 24 is  0.20833333333333331\n",
      "loss in round 26 is  0.23076923076923078\n",
      "loss in round 28 is  0.3214285714285714\n",
      "loss in round 30 is  0.2\n",
      "loss in round 32 is  0.25\n",
      "loss in round 34 is  0.14705882352941177\n",
      "loss in round 36 is  0.2777777777777778\n",
      "loss in round 38 is  0.15789473684210525\n",
      "loss in round 40 is  0.15000000000000002\n",
      "loss in round 42 is  0.14285714285714285\n",
      "loss in round 44 is  0.29545454545454547\n",
      "loss in round 46 is  0.10869565217391304\n",
      "loss in round 48 is  0.25\n",
      "loss in round 50 is  0.2\n",
      "loss in round 52 is  0.25\n",
      "loss in round 54 is  0.25925925925925924\n",
      "loss in round 56 is  0.23214285714285712\n",
      "loss in round 58 is  0.20689655172413793\n",
      "loss in round 60 is  0.25\n",
      "loss in round 62 is  0.25806451612903225\n",
      "loss in round 64 is  0.25\n",
      "loss in round 66 is  0.24242424242424243\n",
      "loss in round 68 is  0.23529411764705882\n",
      "loss in round 70 is  0.18571428571428572\n",
      "loss in round 72 is  0.16666666666666666\n",
      "loss in round 74 is  0.16216216216216217\n",
      "loss in round 76 is  0.23684210526315788\n",
      "loss in round 78 is  0.1923076923076923\n",
      "loss in round 80 is  0.125\n",
      "loss in round 82 is  0.20731707317073172\n",
      "loss in round 84 is  0.26190476190476186\n",
      "loss in round 86 is  0.2558139534883721\n",
      "loss in round 88 is  0.25\n",
      "loss in round 90 is  0.24444444444444446\n",
      "loss in round 92 is  0.25\n",
      "loss in round 94 is  0.1702127659574468\n",
      "loss in round 96 is  0.19791666666666666\n",
      "loss in round 98 is  0.24489795918367346\n",
      "loss in round 100 is  0.24\n",
      "loss in round 102 is  0.3333333333333333\n",
      "loss in round 104 is  0.3076923076923077\n",
      "loss in round 106 is  0.29245283018867924\n",
      "loss in round 108 is  0.28703703703703703\n",
      "loss in round 110 is  0.2636363636363636\n",
      "loss in round 112 is  0.2232142857142857\n",
      "loss in round 114 is  0.21929824561403508\n",
      "loss in round 116 is  0.2672413793103448\n",
      "loss in round 118 is  0.2627118644067797\n",
      "loss in round 120 is  0.25\n",
      "loss in round 122 is  0.30327868852459017\n",
      "loss in round 124 is  0.29838709677419356\n",
      "loss in round 126 is  0.2857142857142857\n",
      "loss in round 128 is  0.1875\n",
      "loss in round 130 is  0.23846153846153847\n",
      "loss in round 132 is  0.26515151515151514\n",
      "loss in round 134 is  0.2462686567164179\n",
      "loss in round 136 is  0.1838235294117647\n",
      "loss in round 138 is  0.1956521739130435\n",
      "loss in round 140 is  0.20714285714285713\n",
      "loss in round 142 is  0.19014084507042253\n",
      "loss in round 144 is  0.21527777777777776\n",
      "loss in round 146 is  0.18493150684931506\n",
      "loss in round 148 is  0.20270270270270271\n",
      "loss in round 150 is  0.19333333333333336\n",
      "loss in round 152 is  0.26973684210526316\n",
      "loss in round 154 is  0.28571428571428575\n",
      "loss in round 156 is  0.23076923076923075\n",
      "loss in round 158 is  0.2721518987341772\n",
      "loss in round 160 is  0.26875\n",
      "loss in round 162 is  0.2345679012345679\n",
      "loss in round 164 is  0.2378048780487805\n",
      "loss in round 166 is  0.18072289156626506\n",
      "loss in round 168 is  0.17857142857142855\n",
      "loss in round 170 is  0.18823529411764706\n",
      "loss in round 172 is  0.18604651162790697\n",
      "loss in round 174 is  0.20114942528735633\n",
      "loss in round 176 is  0.20454545454545456\n",
      "loss in round 178 is  0.1797752808988764\n",
      "loss in round 180 is  0.20555555555555557\n",
      "loss in round 182 is  0.2417582417582418\n",
      "loss in round 184 is  0.22826086956521738\n",
      "loss in round 186 is  0.24731182795698928\n",
      "loss in round 188 is  0.23936170212765956\n",
      "loss in round 190 is  0.18421052631578946\n",
      "loss in round 192 is  0.1875\n",
      "loss in round 194 is  0.22164948453608246\n",
      "loss in round 196 is  0.260204081632653\n",
      "loss in round 198 is  0.24242424242424243\n",
      "loss in round 200 is  0.23\n",
      "loss in round 202 is  0.24752475247524752\n",
      "loss in round 204 is  0.19117647058823528\n",
      "loss in round 206 is  0.2233009708737864\n",
      "loss in round 208 is  0.21153846153846156\n",
      "loss in round 210 is  0.22380952380952382\n",
      "loss in round 212 is  0.20283018867924527\n",
      "loss in round 214 is  0.205607476635514\n",
      "loss in round 216 is  0.20833333333333331\n",
      "loss in round 218 is  0.2018348623853211\n",
      "loss in round 220 is  0.16818181818181818\n",
      "loss in round 222 is  0.1891891891891892\n",
      "loss in round 224 is  0.1875\n",
      "loss in round 226 is  0.2345132743362832\n",
      "loss in round 228 is  0.20175438596491227\n",
      "loss in round 230 is  0.21739130434782608\n",
      "loss in round 232 is  0.21551724137931033\n",
      "loss in round 234 is  0.22649572649572652\n",
      "loss in round 236 is  0.21610169491525424\n",
      "loss in round 238 is  0.2184873949579832\n",
      "loss in round 240 is  0.225\n",
      "loss in round 242 is  0.19421487603305787\n",
      "loss in round 244 is  0.20901639344262296\n",
      "loss in round 246 is  0.21544715447154475\n",
      "loss in round 248 is  0.1935483870967742\n",
      "loss in round 250 is  0.176\n",
      "loss in round 252 is  0.18253968253968253\n",
      "loss in round 254 is  0.15748031496062992\n",
      "loss in round 256 is  0.15625\n",
      "loss in round 258 is  0.2131782945736434\n",
      "loss in round 260 is  0.16923076923076924\n",
      "loss in round 262 is  0.15267175572519084\n",
      "loss in round 264 is  0.1856060606060606\n",
      "loss in round 266 is  0.20676691729323307\n",
      "loss in round 268 is  0.23507462686567163\n",
      "loss in round 270 is  0.21481481481481482\n",
      "loss in round 272 is  0.1875\n",
      "loss in round 274 is  0.218978102189781\n",
      "loss in round 276 is  0.19202898550724637\n",
      "loss in round 278 is  0.18345323741007194\n",
      "loss in round 280 is  0.18214285714285713\n",
      "loss in round 282 is  0.1773049645390071\n",
      "loss in round 284 is  0.17605633802816903\n",
      "loss in round 286 is  0.2097902097902098\n",
      "loss in round 288 is  0.16666666666666666\n",
      "loss in round 290 is  0.2206896551724138\n",
      "loss in round 292 is  0.21232876712328766\n",
      "loss in round 294 is  0.19387755102040816\n",
      "loss in round 296 is  0.1587837837837838\n",
      "loss in round 298 is  0.17114093959731544\n",
      "loss in round 300 is  0.16333333333333333\n",
      "loss in round 302 is  0.17549668874172186\n",
      "loss in round 304 is  0.17434210526315788\n",
      "loss in round 306 is  0.18300653594771243\n",
      "loss in round 308 is  0.15259740259740262\n",
      "loss in round 310 is  0.15161290322580645\n",
      "loss in round 312 is  0.17307692307692307\n",
      "loss in round 314 is  0.15923566878980894\n",
      "loss in round 316 is  0.13924050632911392\n",
      "loss in round 318 is  0.13836477987421383\n",
      "loss in round 320 is  0.14375000000000002\n",
      "loss in round 322 is  0.17391304347826086\n",
      "loss in round 324 is  0.1728395061728395\n",
      "loss in round 326 is  0.16564417177914112\n",
      "loss in round 328 is  0.16463414634146342\n",
      "loss in round 330 is  0.18181818181818182\n",
      "loss in round 332 is  0.16867469879518074\n",
      "loss in round 334 is  0.16766467065868265\n",
      "loss in round 336 is  0.19345238095238093\n",
      "loss in round 338 is  0.1923076923076923\n",
      "loss in round 340 is  0.19705882352941176\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-56768ad1507c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniformmulticlassalgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-1e6d3799d1e9>\u001b[0m in \u001b[0;36muniformmulticlassalgo\u001b[0;34m(feature, y, maxiter, d)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mybart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0my_bar1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_bar1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-1e6d3799d1e9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mybart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0my_bar1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_bar1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss3,m1=uniformmulticlassalgo(x,y,len(y),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/22n0457/imdb/loss3.csv',loss3, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "biological-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in round 2 is  1.0\n",
      "loss in round 4 is  0.5\n",
      "loss in round 6 is  0.8333333333333333\n",
      "loss in round 8 is  0.875\n",
      "loss in round 10 is  0.7000000000000001\n",
      "loss in round 12 is  0.8333333333333333\n",
      "loss in round 14 is  0.7142857142857142\n",
      "loss in round 16 is  0.6875\n",
      "loss in round 18 is  0.611111111111111\n",
      "loss in round 20 is  0.55\n",
      "loss in round 22 is  0.5\n",
      "loss in round 24 is  0.875\n",
      "loss in round 26 is  0.7307692307692308\n",
      "loss in round 28 is  0.6785714285714285\n",
      "loss in round 30 is  0.6666666666666666\n",
      "loss in round 32 is  0.625\n",
      "loss in round 34 is  0.5882352941176471\n",
      "loss in round 36 is  0.611111111111111\n",
      "loss in round 38 is  0.5\n",
      "loss in round 40 is  0.6000000000000001\n",
      "loss in round 42 is  0.5238095238095237\n",
      "loss in round 44 is  0.5\n",
      "loss in round 46 is  0.5217391304347826\n",
      "loss in round 48 is  0.5\n",
      "loss in round 50 is  0.54\n",
      "loss in round 52 is  0.5\n",
      "loss in round 54 is  0.48148148148148145\n",
      "loss in round 56 is  0.46428571428571425\n",
      "loss in round 58 is  0.48275862068965514\n",
      "loss in round 60 is  0.5333333333333333\n",
      "loss in round 62 is  0.4838709677419355\n",
      "loss in round 64 is  0.53125\n",
      "loss in round 66 is  0.5151515151515151\n",
      "loss in round 68 is  0.47058823529411764\n",
      "loss in round 70 is  0.44285714285714284\n",
      "loss in round 72 is  0.4722222222222222\n",
      "loss in round 74 is  0.4864864864864865\n",
      "loss in round 76 is  0.5263157894736842\n",
      "loss in round 78 is  0.5128205128205128\n",
      "loss in round 80 is  0.4625\n",
      "loss in round 82 is  0.45121951219512196\n",
      "loss in round 84 is  0.38095238095238093\n",
      "loss in round 86 is  0.4069767441860465\n",
      "loss in round 88 is  0.5227272727272727\n",
      "loss in round 90 is  0.5666666666666667\n",
      "loss in round 92 is  0.46739130434782605\n",
      "loss in round 94 is  0.5957446808510638\n",
      "loss in round 96 is  0.46875\n",
      "loss in round 98 is  0.520408163265306\n",
      "loss in round 100 is  0.51\n",
      "loss in round 102 is  0.5196078431372549\n",
      "loss in round 104 is  0.5096153846153847\n",
      "loss in round 106 is  0.4716981132075472\n",
      "loss in round 108 is  0.4722222222222222\n",
      "loss in round 110 is  0.5909090909090908\n",
      "loss in round 112 is  0.5982142857142857\n",
      "loss in round 114 is  0.587719298245614\n",
      "loss in round 116 is  0.5086206896551724\n",
      "loss in round 118 is  0.4661016949152542\n",
      "loss in round 120 is  0.475\n",
      "loss in round 122 is  0.49180327868852464\n",
      "loss in round 124 is  0.4838709677419355\n",
      "loss in round 126 is  0.47619047619047616\n",
      "loss in round 128 is  0.515625\n",
      "loss in round 130 is  0.49230769230769234\n",
      "loss in round 132 is  0.5\n",
      "loss in round 134 is  0.47761194029850745\n",
      "loss in round 136 is  0.5294117647058824\n",
      "loss in round 138 is  0.5217391304347826\n",
      "loss in round 140 is  0.65\n",
      "loss in round 142 is  0.6408450704225352\n",
      "loss in round 144 is  0.673611111111111\n",
      "loss in round 146 is  0.6643835616438356\n",
      "loss in round 148 is  0.6351351351351352\n",
      "loss in round 150 is  0.6266666666666667\n",
      "loss in round 152 is  0.7171052631578947\n",
      "loss in round 154 is  0.7272727272727273\n",
      "loss in round 156 is  0.6602564102564102\n",
      "loss in round 158 is  0.7341772151898734\n",
      "loss in round 160 is  0.7250000000000001\n",
      "loss in round 162 is  0.7777777777777777\n",
      "loss in round 164 is  0.774390243902439\n",
      "loss in round 166 is  0.7771084337349398\n",
      "loss in round 168 is  0.7678571428571428\n",
      "loss in round 170 is  0.7529411764705882\n",
      "loss in round 172 is  0.7616279069767442\n",
      "loss in round 174 is  0.6609195402298851\n",
      "loss in round 176 is  0.6761363636363636\n",
      "loss in round 178 is  0.7022471910112359\n",
      "loss in round 180 is  0.7444444444444445\n",
      "loss in round 182 is  0.7857142857142858\n",
      "loss in round 184 is  0.717391304347826\n",
      "loss in round 186 is  0.6612903225806452\n",
      "loss in round 188 is  0.6702127659574468\n",
      "loss in round 190 is  0.6631578947368421\n",
      "loss in round 192 is  0.5885416666666666\n",
      "loss in round 194 is  0.5309278350515464\n",
      "loss in round 196 is  0.5510204081632653\n",
      "loss in round 198 is  0.5808080808080809\n",
      "loss in round 200 is  0.5750000000000001\n",
      "loss in round 202 is  0.5693069306930694\n",
      "loss in round 204 is  0.5686274509803921\n",
      "loss in round 206 is  0.5533980582524272\n",
      "loss in round 208 is  0.5913461538461539\n",
      "loss in round 210 is  0.6047619047619048\n",
      "loss in round 212 is  0.5943396226415094\n",
      "loss in round 214 is  0.5654205607476636\n",
      "loss in round 216 is  0.5694444444444444\n",
      "loss in round 218 is  0.5688073394495413\n",
      "loss in round 220 is  0.5454545454545454\n",
      "loss in round 222 is  0.5630630630630631\n",
      "loss in round 224 is  0.5848214285714285\n",
      "loss in round 226 is  0.5796460176991151\n",
      "loss in round 228 is  0.5789473684210527\n",
      "loss in round 230 is  0.591304347826087\n",
      "loss in round 232 is  0.5560344827586207\n",
      "loss in round 234 is  0.5641025641025642\n",
      "loss in round 236 is  0.5508474576271186\n",
      "loss in round 238 is  0.542016806722689\n",
      "loss in round 240 is  0.5541666666666667\n",
      "loss in round 242 is  0.5619834710743802\n",
      "loss in round 244 is  0.5450819672131147\n",
      "loss in round 246 is  0.5447154471544716\n",
      "loss in round 248 is  0.5362903225806451\n",
      "loss in round 250 is  0.536\n",
      "loss in round 252 is  0.5277777777777778\n",
      "loss in round 254 is  0.5196850393700787\n",
      "loss in round 256 is  0.51953125\n",
      "loss in round 258 is  0.5426356589147286\n",
      "loss in round 260 is  0.5692307692307692\n",
      "loss in round 262 is  0.5419847328244275\n",
      "loss in round 264 is  0.5113636363636364\n",
      "loss in round 266 is  0.5225563909774436\n",
      "loss in round 268 is  0.5261194029850746\n",
      "loss in round 270 is  0.5111111111111112\n",
      "loss in round 272 is  0.5257352941176471\n",
      "loss in round 274 is  0.5182481751824818\n",
      "loss in round 276 is  0.5217391304347826\n",
      "loss in round 278 is  0.5179856115107914\n",
      "loss in round 280 is  0.5357142857142857\n",
      "loss in round 282 is  0.5531914893617021\n",
      "loss in round 284 is  0.5316901408450704\n",
      "loss in round 286 is  0.548951048951049\n",
      "loss in round 288 is  0.5659722222222222\n",
      "loss in round 290 is  0.5551724137931034\n",
      "loss in round 292 is  0.547945205479452\n",
      "loss in round 294 is  0.5612244897959183\n",
      "loss in round 296 is  0.5574324324324325\n",
      "loss in round 298 is  0.5167785234899329\n",
      "loss in round 300 is  0.5466666666666667\n",
      "loss in round 302 is  0.5496688741721855\n",
      "loss in round 304 is  0.6217105263157895\n",
      "loss in round 306 is  0.6176470588235294\n",
      "loss in round 308 is  0.5941558441558442\n",
      "loss in round 310 is  0.6064516129032258\n",
      "loss in round 312 is  0.6025641025641025\n",
      "loss in round 314 is  0.6114649681528663\n",
      "loss in round 316 is  0.5822784810126582\n",
      "loss in round 318 is  0.6257861635220127\n",
      "loss in round 320 is  0.671875\n",
      "loss in round 322 is  0.6801242236024845\n",
      "loss in round 324 is  0.6635802469135802\n",
      "loss in round 326 is  0.6288343558282209\n",
      "loss in round 328 is  0.625\n",
      "loss in round 330 is  0.6303030303030303\n",
      "loss in round 332 is  0.6355421686746988\n",
      "loss in round 334 is  0.6467065868263473\n",
      "loss in round 336 is  0.6547619047619048\n",
      "loss in round 338 is  0.6538461538461539\n",
      "loss in round 340 is  0.6676470588235294\n",
      "loss in round 342 is  0.6637426900584795\n",
      "loss in round 344 is  0.6627906976744186\n",
      "loss in round 346 is  0.6560693641618497\n",
      "loss in round 348 is  0.6637931034482758\n",
      "loss in round 350 is  0.6714285714285714\n",
      "loss in round 352 is  0.6477272727272727\n",
      "loss in round 354 is  0.6497175141242938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-6b053823f6c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworstmulticlassalgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-9839a292462e>\u001b[0m in \u001b[0;36mworstmulticlassalgo\u001b[0;34m(feature, y, maxiter, d)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mybart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0my_bar1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_bar1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \"\"\"\n\u001b[0;32m-> 1188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss4,m2=worstmulticlassalgo(x,y,len(y),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/22n0457/imdb/loss4.csv',loss4, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "durable-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in round 2 is  0.0\n",
      "loss in round 4 is  0.0\n",
      "loss in round 6 is  0.6666666666666666\n",
      "loss in round 8 is  0.5\n",
      "loss in round 10 is  0.5\n",
      "loss in round 12 is  0.41666666666666663\n",
      "loss in round 14 is  0.5\n",
      "loss in round 16 is  0.625\n",
      "loss in round 18 is  0.38888888888888884\n",
      "loss in round 20 is  0.25\n",
      "loss in round 22 is  0.13636363636363635\n",
      "loss in round 24 is  0.125\n",
      "loss in round 26 is  0.15384615384615385\n",
      "loss in round 28 is  0.2857142857142857\n",
      "loss in round 30 is  0.2\n",
      "loss in round 32 is  0.25\n",
      "loss in round 34 is  0.23529411764705882\n",
      "loss in round 36 is  0.2777777777777778\n",
      "loss in round 38 is  0.2631578947368421\n",
      "loss in round 40 is  0.25\n",
      "loss in round 42 is  0.21428571428571427\n",
      "loss in round 44 is  0.2727272727272727\n",
      "loss in round 46 is  0.19565217391304346\n",
      "loss in round 48 is  0.2708333333333333\n",
      "loss in round 50 is  0.28\n",
      "loss in round 52 is  0.21153846153846156\n",
      "loss in round 54 is  0.2222222222222222\n",
      "loss in round 56 is  0.19642857142857142\n",
      "loss in round 58 is  0.27586206896551724\n",
      "loss in round 60 is  0.26666666666666666\n",
      "loss in round 62 is  0.27419354838709675\n",
      "loss in round 64 is  0.28125\n",
      "loss in round 66 is  0.2727272727272727\n",
      "loss in round 68 is  0.22058823529411764\n",
      "loss in round 70 is  0.14285714285714285\n",
      "loss in round 72 is  0.2638888888888889\n",
      "loss in round 74 is  0.2567567567567568\n",
      "loss in round 76 is  0.25\n",
      "loss in round 78 is  0.24358974358974358\n",
      "loss in round 80 is  0.225\n",
      "loss in round 82 is  0.1951219512195122\n",
      "loss in round 84 is  0.2738095238095238\n",
      "loss in round 86 is  0.2441860465116279\n",
      "loss in round 88 is  0.23863636363636365\n",
      "loss in round 90 is  0.22222222222222224\n",
      "loss in round 92 is  0.25\n",
      "loss in round 94 is  0.22340425531914893\n",
      "loss in round 96 is  0.3020833333333333\n",
      "loss in round 98 is  0.2857142857142857\n",
      "loss in round 100 is  0.29\n",
      "loss in round 102 is  0.3333333333333333\n",
      "loss in round 104 is  0.2884615384615385\n",
      "loss in round 106 is  0.32075471698113206\n",
      "loss in round 108 is  0.3055555555555555\n",
      "loss in round 110 is  0.2909090909090909\n",
      "loss in round 112 is  0.25\n",
      "loss in round 114 is  0.2982456140350877\n",
      "loss in round 116 is  0.29310344827586204\n",
      "loss in round 118 is  0.2627118644067797\n",
      "loss in round 120 is  0.23333333333333334\n",
      "loss in round 122 is  0.26229508196721313\n",
      "loss in round 124 is  0.2661290322580645\n",
      "loss in round 126 is  0.2698412698412698\n",
      "loss in round 128 is  0.2734375\n",
      "loss in round 130 is  0.24615384615384617\n",
      "loss in round 132 is  0.24242424242424243\n",
      "loss in round 134 is  0.23880597014925373\n",
      "loss in round 136 is  0.25735294117647056\n",
      "loss in round 138 is  0.2826086956521739\n",
      "loss in round 140 is  0.2642857142857143\n",
      "loss in round 142 is  0.26760563380281693\n",
      "loss in round 144 is  0.29166666666666663\n",
      "loss in round 146 is  0.2876712328767123\n",
      "loss in round 148 is  0.28378378378378377\n",
      "loss in round 150 is  0.26666666666666666\n",
      "loss in round 152 is  0.29605263157894735\n",
      "loss in round 154 is  0.3116883116883117\n",
      "loss in round 156 is  0.2564102564102564\n",
      "loss in round 158 is  0.2721518987341772\n",
      "loss in round 160 is  0.26875\n",
      "loss in round 162 is  0.24691358024691357\n",
      "loss in round 164 is  0.2378048780487805\n",
      "loss in round 166 is  0.24698795180722893\n",
      "loss in round 168 is  0.25\n",
      "loss in round 170 is  0.22941176470588234\n",
      "loss in round 172 is  0.21511627906976744\n",
      "loss in round 174 is  0.1896551724137931\n",
      "loss in round 176 is  0.2556818181818182\n",
      "loss in round 178 is  0.2584269662921348\n",
      "loss in round 180 is  0.23333333333333334\n",
      "loss in round 182 is  0.2197802197802198\n",
      "loss in round 184 is  0.2608695652173913\n",
      "loss in round 186 is  0.23655913978494625\n",
      "loss in round 188 is  0.2127659574468085\n",
      "loss in round 190 is  0.21578947368421053\n",
      "loss in round 192 is  0.21354166666666666\n",
      "loss in round 194 is  0.26804123711340205\n",
      "loss in round 196 is  0.2755102040816326\n",
      "loss in round 198 is  0.2676767676767677\n",
      "loss in round 200 is  0.275\n",
      "loss in round 202 is  0.2722772277227723\n",
      "loss in round 204 is  0.3088235294117647\n",
      "loss in round 206 is  0.2815533980582524\n",
      "loss in round 208 is  0.2644230769230769\n",
      "loss in round 210 is  0.2761904761904762\n",
      "loss in round 212 is  0.24528301886792453\n",
      "loss in round 214 is  0.24299065420560745\n",
      "loss in round 216 is  0.24537037037037035\n",
      "loss in round 218 is  0.24311926605504589\n",
      "loss in round 220 is  0.2409090909090909\n",
      "loss in round 222 is  0.24324324324324323\n",
      "loss in round 224 is  0.20535714285714285\n",
      "loss in round 226 is  0.25663716814159293\n",
      "loss in round 228 is  0.19298245614035087\n",
      "loss in round 230 is  0.19130434782608696\n",
      "loss in round 232 is  0.1896551724137931\n",
      "loss in round 234 is  0.2435897435897436\n",
      "loss in round 236 is  0.2584745762711864\n",
      "loss in round 238 is  0.2394957983193277\n",
      "loss in round 240 is  0.2583333333333333\n",
      "loss in round 242 is  0.21074380165289258\n",
      "loss in round 244 is  0.2377049180327869\n",
      "loss in round 246 is  0.23577235772357727\n",
      "loss in round 248 is  0.22983870967741934\n",
      "loss in round 250 is  0.244\n",
      "loss in round 252 is  0.2777777777777778\n",
      "loss in round 254 is  0.23622047244094488\n",
      "loss in round 256 is  0.26171875\n",
      "loss in round 258 is  0.23255813953488372\n",
      "loss in round 260 is  0.21153846153846156\n",
      "loss in round 262 is  0.24427480916030533\n",
      "loss in round 264 is  0.2196969696969697\n",
      "loss in round 266 is  0.23308270676691728\n",
      "loss in round 268 is  0.24253731343283583\n",
      "loss in round 270 is  0.24074074074074076\n",
      "loss in round 272 is  0.22426470588235295\n",
      "loss in round 274 is  0.22627737226277372\n",
      "loss in round 276 is  0.2427536231884058\n",
      "loss in round 278 is  0.2302158273381295\n",
      "loss in round 280 is  0.2464285714285714\n",
      "loss in round 282 is  0.22340425531914893\n",
      "loss in round 284 is  0.2112676056338028\n",
      "loss in round 286 is  0.1958041958041958\n",
      "loss in round 288 is  0.21527777777777776\n",
      "loss in round 290 is  0.21379310344827585\n",
      "loss in round 292 is  0.1952054794520548\n",
      "loss in round 294 is  0.19387755102040816\n",
      "loss in round 296 is  0.22297297297297297\n",
      "loss in round 298 is  0.24161073825503354\n",
      "loss in round 300 is  0.27\n",
      "loss in round 302 is  0.23841059602649006\n",
      "loss in round 304 is  0.23026315789473684\n",
      "loss in round 306 is  0.21568627450980393\n",
      "loss in round 308 is  0.18181818181818182\n",
      "loss in round 310 is  0.18064516129032257\n",
      "loss in round 312 is  0.16987179487179488\n",
      "loss in round 314 is  0.18789808917197454\n",
      "loss in round 316 is  0.2120253164556962\n",
      "loss in round 318 is  0.21069182389937108\n",
      "loss in round 320 is  0.21875\n",
      "loss in round 322 is  0.23602484472049687\n",
      "loss in round 324 is  0.25308641975308643\n",
      "loss in round 326 is  0.2392638036809816\n",
      "loss in round 328 is  0.2378048780487805\n",
      "loss in round 330 is  0.23636363636363636\n",
      "loss in round 332 is  0.23795180722891568\n",
      "loss in round 334 is  0.23652694610778444\n",
      "loss in round 336 is  0.22916666666666666\n",
      "loss in round 338 is  0.21301775147928995\n",
      "loss in round 340 is  0.2\n",
      "loss in round 342 is  0.19005847953216373\n",
      "loss in round 344 is  0.18895348837209303\n",
      "loss in round 346 is  0.23121387283236994\n",
      "loss in round 348 is  0.2557471264367816\n",
      "loss in round 350 is  0.26\n",
      "loss in round 352 is  0.29829545454545453\n",
      "loss in round 354 is  0.3107344632768362\n",
      "loss in round 356 is  0.22752808988764045\n",
      "loss in round 358 is  0.22625698324022347\n",
      "loss in round 360 is  0.24722222222222223\n",
      "loss in round 362 is  0.23756906077348067\n",
      "loss in round 364 is  0.20604395604395606\n",
      "loss in round 366 is  0.20491803278688525\n",
      "loss in round 368 is  0.19293478260869565\n",
      "loss in round 370 is  0.1837837837837838\n",
      "loss in round 372 is  0.17204301075268819\n",
      "loss in round 374 is  0.18716577540106952\n",
      "loss in round 376 is  0.2154255319148936\n",
      "loss in round 378 is  0.26455026455026454\n",
      "loss in round 380 is  0.25789473684210523\n",
      "loss in round 382 is  0.2329842931937173\n",
      "loss in round 384 is  0.2265625\n",
      "loss in round 386 is  0.2461139896373057\n",
      "loss in round 388 is  0.1958762886597938\n",
      "loss in round 390 is  0.2\n",
      "loss in round 392 is  0.2423469387755102\n",
      "loss in round 394 is  0.20558375634517764\n",
      "loss in round 396 is  0.20454545454545456\n",
      "loss in round 398 is  0.20100502512562815\n",
      "loss in round 400 is  0.2\n",
      "loss in round 402 is  0.1990049751243781\n",
      "loss in round 404 is  0.2103960396039604\n",
      "loss in round 406 is  0.18472906403940886\n",
      "loss in round 408 is  0.1838235294117647\n",
      "loss in round 410 is  0.17804878048780487\n",
      "loss in round 412 is  0.17233009708737862\n",
      "loss in round 414 is  0.16183574879227053\n",
      "loss in round 416 is  0.17307692307692307\n",
      "loss in round 418 is  0.18660287081339713\n",
      "loss in round 420 is  0.17380952380952383\n",
      "loss in round 422 is  0.1966824644549763\n",
      "loss in round 424 is  0.19575471698113206\n",
      "loss in round 426 is  0.19248826291079812\n",
      "loss in round 428 is  0.1985981308411215\n",
      "loss in round 430 is  0.22093023255813954\n",
      "loss in round 432 is  0.23148148148148145\n",
      "loss in round 434 is  0.2350230414746544\n",
      "loss in round 436 is  0.22706422018348627\n",
      "loss in round 438 is  0.21232876712328766\n",
      "loss in round 440 is  0.1909090909090909\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-9dd8dcef6ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvimulticlassalgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-1c04923bb553>\u001b[0m in \u001b[0;36mvimulticlassalgo\u001b[0;34m(feature, y, maxiter, d)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mybart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0my_bar1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_bar1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-1c04923bb553>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mybart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0my_bar1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_bar1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss5,m3=vimulticlassalgo(x,y,len(y),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/22n0457/imdb/loss5.csv',loss5, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "offensive-craft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in round 2 is  0.0\n",
      "loss in round 4 is  0.5\n",
      "loss in round 6 is  0.8333333333333333\n",
      "loss in round 8 is  0.625\n",
      "loss in round 10 is  0.4\n",
      "loss in round 12 is  0.3333333333333333\n",
      "loss in round 14 is  0.5\n",
      "loss in round 16 is  0.25\n",
      "loss in round 18 is  0.2777777777777778\n",
      "loss in round 20 is  0.30000000000000004\n",
      "loss in round 22 is  0.4090909090909091\n",
      "loss in round 24 is  0.3333333333333333\n",
      "loss in round 26 is  0.2692307692307693\n",
      "loss in round 28 is  0.3571428571428571\n",
      "loss in round 30 is  0.43333333333333335\n",
      "loss in round 32 is  0.375\n",
      "loss in round 34 is  0.4411764705882353\n",
      "loss in round 36 is  0.2777777777777778\n",
      "loss in round 38 is  0.42105263157894735\n",
      "loss in round 40 is  0.375\n",
      "loss in round 42 is  0.42857142857142855\n",
      "loss in round 44 is  0.29545454545454547\n",
      "loss in round 46 is  0.45652173913043476\n",
      "loss in round 48 is  0.2708333333333333\n",
      "loss in round 50 is  0.46\n",
      "loss in round 52 is  0.46153846153846156\n",
      "loss in round 54 is  0.38888888888888884\n",
      "loss in round 56 is  0.4821428571428571\n",
      "loss in round 58 is  0.43103448275862066\n",
      "loss in round 60 is  0.4166666666666667\n",
      "loss in round 62 is  0.3709677419354839\n",
      "loss in round 64 is  0.375\n",
      "loss in round 66 is  0.4090909090909091\n",
      "loss in round 68 is  0.4117647058823529\n",
      "loss in round 70 is  0.45714285714285713\n",
      "loss in round 72 is  0.4861111111111111\n",
      "loss in round 74 is  0.43243243243243246\n",
      "loss in round 76 is  0.48684210526315785\n",
      "loss in round 78 is  0.5769230769230769\n",
      "loss in round 80 is  0.5\n",
      "loss in round 82 is  0.475609756097561\n",
      "loss in round 84 is  0.42857142857142855\n",
      "loss in round 86 is  0.5232558139534884\n",
      "loss in round 88 is  0.5113636363636364\n",
      "loss in round 90 is  0.5\n",
      "loss in round 92 is  0.5108695652173912\n",
      "loss in round 94 is  0.5531914893617021\n",
      "loss in round 96 is  0.5208333333333333\n",
      "loss in round 98 is  0.4897959183673469\n",
      "loss in round 100 is  0.44\n",
      "loss in round 102 is  0.49019607843137253\n",
      "loss in round 104 is  0.5576923076923077\n",
      "loss in round 106 is  0.49056603773584906\n",
      "loss in round 108 is  0.5277777777777778\n",
      "loss in round 110 is  0.43636363636363634\n",
      "loss in round 112 is  0.5178571428571428\n",
      "loss in round 114 is  0.47368421052631576\n",
      "loss in round 116 is  0.41379310344827586\n",
      "loss in round 118 is  0.4322033898305085\n",
      "loss in round 120 is  0.4083333333333333\n",
      "loss in round 122 is  0.4016393442622951\n",
      "loss in round 124 is  0.4193548387096774\n",
      "loss in round 126 is  0.38888888888888884\n",
      "loss in round 128 is  0.4375\n",
      "loss in round 130 is  0.4307692307692308\n",
      "loss in round 132 is  0.40151515151515155\n",
      "loss in round 134 is  0.3805970149253731\n",
      "loss in round 136 is  0.40441176470588236\n",
      "loss in round 138 is  0.4420289855072464\n",
      "loss in round 140 is  0.47857142857142854\n",
      "loss in round 142 is  0.471830985915493\n",
      "loss in round 144 is  0.5\n",
      "loss in round 146 is  0.5\n",
      "loss in round 148 is  0.4594594594594595\n",
      "loss in round 150 is  0.42000000000000004\n",
      "loss in round 152 is  0.4473684210526315\n",
      "loss in round 154 is  0.4545454545454546\n",
      "loss in round 156 is  0.46794871794871795\n",
      "loss in round 158 is  0.49367088607594933\n",
      "loss in round 160 is  0.5\n",
      "loss in round 162 is  0.4629629629629629\n",
      "loss in round 164 is  0.43902439024390244\n",
      "loss in round 166 is  0.5120481927710844\n",
      "loss in round 168 is  0.5238095238095237\n",
      "loss in round 170 is  0.5058823529411764\n",
      "loss in round 172 is  0.46511627906976744\n",
      "loss in round 174 is  0.46551724137931033\n",
      "loss in round 176 is  0.5113636363636364\n",
      "loss in round 178 is  0.4887640449438202\n",
      "loss in round 180 is  0.5333333333333333\n",
      "loss in round 182 is  0.5054945054945056\n",
      "loss in round 184 is  0.483695652173913\n",
      "loss in round 186 is  0.5752688172043011\n",
      "loss in round 188 is  0.5585106382978723\n",
      "loss in round 190 is  0.6105263157894737\n",
      "loss in round 192 is  0.5572916666666666\n",
      "loss in round 194 is  0.5257731958762887\n",
      "loss in round 196 is  0.5255102040816326\n",
      "loss in round 198 is  0.48989898989898994\n",
      "loss in round 200 is  0.55\n",
      "loss in round 202 is  0.5247524752475248\n",
      "loss in round 204 is  0.5490196078431373\n",
      "loss in round 206 is  0.5825242718446602\n",
      "loss in round 208 is  0.5817307692307693\n",
      "loss in round 210 is  0.5571428571428572\n",
      "loss in round 212 is  0.6132075471698113\n",
      "loss in round 214 is  0.5981308411214953\n",
      "loss in round 216 is  0.6342592592592592\n",
      "loss in round 218 is  0.6009174311926606\n",
      "loss in round 220 is  0.5681818181818181\n",
      "loss in round 222 is  0.6171171171171171\n",
      "loss in round 224 is  0.6339285714285714\n",
      "loss in round 226 is  0.6017699115044248\n",
      "loss in round 228 is  0.6096491228070176\n",
      "loss in round 230 is  0.6173913043478261\n",
      "loss in round 232 is  0.5991379310344828\n",
      "loss in round 234 is  0.5598290598290598\n",
      "loss in round 236 is  0.5127118644067796\n",
      "loss in round 238 is  0.49579831932773105\n",
      "loss in round 240 is  0.49583333333333335\n",
      "loss in round 242 is  0.4628099173553719\n",
      "loss in round 244 is  0.45901639344262296\n",
      "loss in round 246 is  0.467479674796748\n",
      "loss in round 248 is  0.42338709677419356\n",
      "loss in round 250 is  0.41200000000000003\n",
      "loss in round 252 is  0.4087301587301587\n",
      "loss in round 254 is  0.41732283464566927\n",
      "loss in round 256 is  0.41796875\n",
      "loss in round 258 is  0.46124031007751937\n",
      "loss in round 260 is  0.5115384615384616\n",
      "loss in round 262 is  0.4618320610687023\n",
      "loss in round 264 is  0.4696969696969697\n",
      "loss in round 266 is  0.47744360902255634\n",
      "loss in round 268 is  0.458955223880597\n",
      "loss in round 270 is  0.4481481481481482\n",
      "loss in round 272 is  0.4632352941176471\n",
      "loss in round 274 is  0.4562043795620438\n",
      "loss in round 276 is  0.4782608695652174\n",
      "loss in round 278 is  0.46402877697841727\n",
      "loss in round 280 is  0.47857142857142854\n",
      "loss in round 282 is  0.5070921985815603\n",
      "loss in round 284 is  0.5\n",
      "loss in round 286 is  0.4965034965034965\n",
      "loss in round 288 is  0.5\n",
      "loss in round 290 is  0.5413793103448276\n",
      "loss in round 292 is  0.5034246575342466\n",
      "loss in round 294 is  0.5\n",
      "loss in round 296 is  0.5270270270270271\n",
      "loss in round 298 is  0.4664429530201342\n",
      "loss in round 300 is  0.43333333333333335\n",
      "loss in round 302 is  0.48013245033112584\n",
      "loss in round 304 is  0.48684210526315785\n",
      "loss in round 306 is  0.4673202614379085\n",
      "loss in round 308 is  0.4675324675324676\n",
      "loss in round 310 is  0.45806451612903226\n",
      "loss in round 312 is  0.48717948717948717\n",
      "loss in round 314 is  0.4968152866242038\n",
      "loss in round 316 is  0.5379746835443038\n",
      "loss in round 318 is  0.5377358490566038\n",
      "loss in round 320 is  0.534375\n",
      "loss in round 322 is  0.5434782608695652\n",
      "loss in round 324 is  0.5123456790123456\n",
      "loss in round 326 is  0.5214723926380368\n",
      "loss in round 328 is  0.5213414634146342\n",
      "loss in round 330 is  0.509090909090909\n",
      "loss in round 332 is  0.4819277108433735\n",
      "loss in round 334 is  0.4940119760479042\n",
      "loss in round 336 is  0.4880952380952381\n",
      "loss in round 338 is  0.4911242603550296\n",
      "loss in round 340 is  0.5147058823529411\n",
      "loss in round 342 is  0.5175438596491228\n",
      "loss in round 344 is  0.5261627906976745\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-92c611ae0c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmira\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-11672b717b2b>\u001b[0m in \u001b[0;36mmira\u001b[0;34m(feature, y, maxiter, d)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mybart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0my_bar1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_bar1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-11672b717b2b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mybart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0my_bar1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_bar1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss6,m4=mira(x,y,len(y),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "specialized-alpha",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-cf64c310114a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/22n0457/imdb/loss6.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss6' is not defined"
     ]
    }
   ],
   "source": [
    "np.savetxt('/home/22n0457/imdb/loss6.csv',loss6, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "comfortable-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pranking_updated(train,y,t,m):\n",
    "    w=np.zeros(train.shape[1])\n",
    "    b = np.zeros(len(np.unique(y)) - 1)\n",
    "    b = np.append(b, np.inf)\n",
    "    loss=[]\n",
    "    label=range(1,len(np.unique(y))+1)\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    y = np.array([np.where(unique_y == labels)[0][0] + 1 for labels in y]) \n",
    "    def predict(x, w, b):\n",
    "        scores = np.absolute(safe_sparse_dot(x, w)-b)\n",
    "        return np.argmin(scores) + 1\n",
    "    for s in range(m):\n",
    "        for i in range(1,t+1):\n",
    "            x=train[i-1]\n",
    "            y_bar=predict(x, w, b)\n",
    "            if y_bar !=y[i-1]:\n",
    "                y_rt=np.where(y[i-1]<=label[:-1],-1,1)\n",
    "                t_rt=np.where((safe_sparse_dot(x,w)-b[:-1])*y_rt <=0,y_rt,0)\n",
    "                w=w+sum(t_rt)*x\n",
    "                b[:-1] -= t_rt\n",
    "            #predicting score\n",
    "            if i%1000==0:\n",
    "                y_bar1=np.array([predict(train[j-1],w,b) for j in range(1,i+1)])\n",
    "                l = np.mean(np.abs(y[:i] - y_bar1)) #loss function\n",
    "                print(f\"loss in round {i} is \",l)\n",
    "                loss.append(l)\n",
    "    return loss,w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "overhead-witness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in round 1000 is  1.659\n",
      "loss in round 1000 is  1.078\n",
      "loss in round 1000 is  1.05\n",
      "loss in round 1000 is  0.55\n",
      "loss in round 1000 is  0.533\n",
      "loss in round 1000 is  0.51\n",
      "loss in round 1000 is  0.467\n",
      "loss in round 1000 is  0.486\n",
      "loss in round 1000 is  0.449\n",
      "loss in round 1000 is  0.433\n",
      "loss in round 1000 is  0.568\n",
      "loss in round 1000 is  0.537\n",
      "loss in round 1000 is  0.535\n",
      "loss in round 1000 is  0.534\n",
      "loss in round 1000 is  0.54\n",
      "loss in round 1000 is  0.4\n",
      "loss in round 1000 is  0.398\n",
      "loss in round 1000 is  0.533\n",
      "loss in round 1000 is  0.533\n",
      "loss in round 1000 is  0.533\n"
     ]
    }
   ],
   "source": [
    "loss7,w21,b21=pranking_updated(x,y,len(y),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/22n0457/imdb/loss7.csv',loss7, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "manual-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import safe_sparse_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hispanic-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalRegressionNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = safe_sparse_dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.sigmoid(self.Z1)\n",
    "        self.Z2 = safe_sparse_dot(self.A1, self.W2) + self.b2\n",
    "        self.O = [self.sigmoid(z) for z in self.Z2]\n",
    "        return self.O\n",
    "\n",
    "    def backward(self, X, y_true, y_pred, learning_rate):\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # calculate errors and gradients\n",
    "        delta2 = y_pred - y_true\n",
    "        dW2 = safe_sparse_dot(self.A1.T, delta2)\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "        delta1 = safe_sparse_dot(delta2, self.W2.T) * (self.A1 * (1 - self.A1))\n",
    "        dW1 = safe_sparse_dot(X.T, delta1)\n",
    "        db1 = np.sum(delta1, axis=0)\n",
    "\n",
    "        # update weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, learning_rate=0.01, epochs=100):\n",
    "        n_samples = X.shape[0]\n",
    "        loss=[]\n",
    "\n",
    "        # encode ordinal categories in target vectors\n",
    "        y_encoded = np.zeros((n_samples, self.output_size))\n",
    "        for i, y_i in enumerate(y):\n",
    "            y_encoded[i, :int(y_i)] = 1\n",
    "\n",
    "        # train the network\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            self.backward(X, y_encoded, y_pred, learning_rate)\n",
    "            y_pred = [np.argmax(o) + 1 for o in y_pred]\n",
    "            loss.append(np.sum(np.absolute(y_pred-y))/len(y))\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X)\n",
    "        y_pred = [np.argmax(o) + 1 for o in y_pred]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "impressive-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=range(1,len(np.unique(y))+1)\n",
    "unique_y = np.sort(np.unique(y))\n",
    "y = np.array([np.where(unique_y == labels)[0][0] + 1 for labels in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "enabling-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OrdinalRegressionNetwork(input_size=np.shape(x)[1], hidden_size=2**7, output_size=int(len(np.unique(y))))\n",
    "#loss=model.train(x, y, learning_rate=0.1, epochs=1000)\n",
    "loss=model.train(x[:100], y[:100], learning_rate=0.1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/22n0457/imdb/loss8.csv',loss, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "indian-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.32]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-adjustment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
