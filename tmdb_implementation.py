# -*- coding: utf-8 -*-
"""tmdb_implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i9rEpEMu6RmpD_qe1zGUC3izebOHpbKj
"""

import pandas as pd
import numpy as np

#To avoid unnecessary warning message
import warnings
warnings.filterwarnings('ignore')

credit_df=pd.read_csv("/home/22n0457/credits.csv")
movie_df=pd.read_csv("/home/22n0457/movies_metadata.csv")
rating=pd.read_csv("/home/22n0457/ratings.csv")
keywords=pd.read_csv("/home/22n0457/keywords.csv")

credit_df.head(5)

credit_df.iloc[0,0]

credit_df.info()

rating.head()

movie_df.head()

keywords.head(2)

movie_df.rename(columns={'id':'movieId'},inplace=True)
movie_df.head(2)

keywords.rename(columns={'id':'movieId'},inplace=True)
keywords['movieId'].dtype

credit_df.rename(columns={'id':'movieId'},inplace=True)
credit_df.head(2)

rating["movieId"].dtype

movie_df

def isint(num):
    try:
        int(num)
        return True
    except ValueError:
        return False

for i in range(movie_df.shape[0]):
    if  isint(movie_df['movieId'][i])==False:
        movie_df.drop(i,axis=0,inplace=True)

movie_df.shape

movie_df['movieId']=movie_df['movieId'].astype('int64')

movie=movie_df.merge(credit_df,on='movieId')
movie=movie.merge(keywords,on='movieId')

movie.head(2)

df=movie

#Dropping irrelevant columns
df.drop(['adult','belongs_to_collection','poster_path'],axis=1,inplace=True)

df.info()

df.isnull().sum()

#since homepage and tagline has many null value so we drop these two column
df.drop(columns=['homepage','tagline','original_language'],inplace=True)
#Since its a large dataset we can drop the rows with missing values as it wont make a huge impact
df.dropna(inplace=True)

df.head()

df.info()

df.isnull().sum()

#Checking for duplicate entries in our data
df.duplicated().sum()

df.drop_duplicates(keep='first',inplace=True)

df['genres']

import ast

#Creating a function to loop over feature and fetch only the details needed to append it in a list

def convert(obj): #Function which takes the feature as input
    l=[] #Empty List
    for i in ast.literal_eval(obj): #Looping over the feature
        l.append(i['name']) #fetching and appending only the name key in the feature
    if len(l)==0:
        return ['']
    return l #returning the list

df['genres']=df['genres'].apply(convert)
df['keywords']=df['keywords'].apply(convert)   #Applying the function to both our genres and keywords features

#Now our genres and keywords feature includes list of names and not a dict
df.iloc[5].genres

df.iloc[2].cast

#Creating another function to fetch the 7 cast names
def convert(obj):#Function
    l=[] #Empty list to append names
    counter=0 #Creating a counter to ensure we add 7 cast names to the list for each movie
    for i in ast.literal_eval(obj): #Looping over the feature
        if counter !=7 : #setting the counter limit to 7
            l.append(i['name']) #appending the cast names to empy list
            counter+=1 #increasing counter value
        else:
            break #Breaking the loop after 7 names have been added to list
    if len(l)==0:
        return ['']
    return l #Returning the list

#Applying the function to cast feature
df['cast']=df['cast'].apply(convert)

df.iloc[2].crew

#Creating a function to fetch the director name from the crew feature
def fetch_director(obj):
    l=[]
    for i in ast.literal_eval(obj):
        if i['job']=='Director':
            l.append(i['name'])
    if len(l)==0:
        return ['']
    return l

#Applying the function
df['crew']=df['crew'].apply(fetch_director)

#Checking if function worked
df.head(2)

#Checking if function worked
df.head(2)

#Splitting the words in overview column to create a list of all the words
df['overview']=df['overview'].apply(lambda x:x.split())

df.head(3)

df.head(3)

df.columns

#taking production company name as list
df['production_companies']=df['production_companies'].apply(convert)

# name of production company
def convert_str(obj):
    for i in ast.literal_eval(obj): #Looping over the feature
        return [i['name']] #fetching and appending only the name key in the feature

df['production_countries']=df['production_countries'].apply(convert_str)

df.info()



df['production_countries'].isnull()

df['production_countries'][df['production_countries'].isnull()]=[[''] for i in range(df['production_countries'].isnull().sum())]

for i in range(df.shape[0]):
    if df.iloc[i,4] !=df.iloc[i,14]:
        df.iloc[i,4]=0   # if title is change from original title then 0 other wise 1
    else:
        df.iloc[i,4]=1

df['spoken_languages'][5]

df['spoken_languages']=df['spoken_languages'].apply(convert)

df['spoken_languages'][5]

# changing imbd id into integer
for i in range(df.shape[0]):
  p=list(df.iloc[i,3])
  df.iloc[i,3]=''.join(p[2:])
df['imdb_id']=df['imdb_id'].astype('int64')

df['release_date'][78]

df['release_date'] = pd.to_datetime(df['release_date'],
 format = '%Y-%m-%d',
 errors = 'coerce')

df['release_day']=df['release_date'].dt.day
df['release_month']=df['release_date'].dt.month
df['release_year']=df['release_date'].dt.year

df.drop(['release_date'],axis=1,inplace=True)

df['status'].unique()

df['status']=df['status'].replace({'Released':1, 'Rumored':2, 'Post Production':3, 'In Production':4,'Planned':5, 'Canceled':6})

df['video'].unique()

df['video']=df['video'].replace({False:0,True:1})

df['status'].unique()

df['video'].unique()

df.info()

k=0
for i in range(df.shape[0]):
    if  isint(df.iloc[i,0])==False:
        k+=1
print(k)

df['budget']=df['budget'].astype('int64')

df['original_title']=df['original_title'].astype('int64')

df['popularity']=df['popularity'].astype('float64')

df.info()

df['production_countries']

df['tags']=df['genres'] + df['overview']+ df['production_companies'] + df['spoken_languages'] + df['cast'] + df['crew'] + df['keywords']

for i in range(df.shape[0]):
    df.iloc[i,23].append(df.iloc[i,13])

df.drop(columns=['title','genres','overview','production_companies','spoken_languages','cast','crew','keywords','production_countries'],inplace=True)

df.shape

rating.shape

df.isnull().sum()

ratings = rating.groupby('movieId').rating.mean().reset_index()

df=df.merge(ratings,on='movieId')

df['tags'][0]

df.head(5)

df['rating']=df['rating'].round(1)

np.sort(df['rating'].unique())

np.argmax([len(df['tags'][i]) for i in range(df.shape[0])])

df['tags'][994]

import nltk

from nltk.corpus import stopwords

for i in range(df.shape[0]):
    df.loc[i,'tags']=" ".join(df.loc[i,'tags'])

from bs4 import BeautifulSoup
import re,string,unicodedata

nltk.download('stopwords')

stop = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stop.update(punctuation)

def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#Removing the square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)
# Removing URL's
def remove_between_square_brackets(text):
    return re.sub(r'http\S+', '', text)
#Removing the stopwords from text
def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stop and i.strip().lower().isalpha():
            final_text.append(i.strip().lower())
    return " ".join(final_text)
#Removing the noisy text
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    text = remove_stopwords(text)
    return text
#Apply function on review column
df['tags']=df['tags'].apply(denoise_text)

def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus = get_corpus(df.tags)
corpus[:5]

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
# Creating a function to perform stemming
def stemming(text):
    s = []
    for words in text.split():
        s.append(ps.stem(words))
    a = s[:]
    s.clear()
    return ' '.join(a)
# Applying that function in review feature to perform stemming
df['tags']=df['tags'].apply(stemming)

def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus = get_corpus(df.tags)
corpus[:5]

y=df['rating'].to_numpy()
df.drop(columns=['movieId','imdb_id','rating'],inplace=True)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=10000)
X = cv.fit_transform(df['tags']).toarray()

"""from sklearn.feature_extraction.text import HashingVectorizer

text = ["The quick brown fox jumped over the lazy dog."]
vectorizer = HashingVectorizer(n_features=20)
vector = vectorizer.transform(text)"""

from sklearn.utils.extmath import safe_sparse_dot

x=X
y=y

# Commented out IPython magic to ensure Python compatibility.
# %cd '/home/divya/vivek5/mlp/'
import algorithm

loss1,w1=algorithm.widrowhoff(x,y,7428,0.001,200)

np.savetxt('/home/22n0457/imdb/loss1.csv',loss1, delimiter=',')
np.savetxt('/home/22n0457/imdb/w1.csv',w1, delimiter=',')

loss2,w2,b2=algorithm.pranking(x,y,7428,20)

np.savetxt('/home/22n0457/imdb/loss2.csv',loss2, delimiter=',')
np.savetxt('/home/22n0457/imdb/w2.csv',w2, delimiter=',')
np.savetxt('/home/22n0457/imdb/b2.csv',b2, delimiter=',')

loss3,m1=algorithm.uniformmulticlassalgo(x,y,7428,200)

np.savetxt('/home/22n0457/imdb/loss3.csv',loss3, delimiter=',')

loss4,m2=algorithm.worstmulticlassalgo(x,y,7428,200)

loss4

np.savetxt('/home/22n0457/imdb/loss4.csv',loss4, delimiter=',')

loss5,m3=algorithm.vimulticlassalgo(x,y,7428,200)

np.savetxt('/home/22n0457/imdb/loss5.csv',loss5, delimiter=',')

loss6,m4=algorithm.mira(x,y,7428,200)

np.savetxt('/home/22n0457/imdb/loss6.csv',loss6, delimiter=',')

loss6

loss21,w21,b21=algorithm.pranking_updated(x,y,7428,20)

label=range(1,len(np.unique(y))+1)
unique_y = np.sort(np.unique(y))
y = np.array([np.where(unique_y == labels)[0][0] + 1 for labels in y])

model = algorithm.OrdinalRegressionNetwork(input_size=10000, hidden_size=2**5, output_size=42)
#loss=model.train(X, y, learning_rate=0.01, epochs=7000)
loss=model.train(X[:100], y[:100], learning_rate=0.01, epochs=1)

print(np.sum(np.absolute(model.predict(X)-y))/len(y))

np.savetxt('/home/22n0457/tmdb/loss7.csv',loss, delimiter=',')

loss21,w21,b21=pranking_updated(x,y,7428,200)

np.savetxt('/home/22n0457/tmdb/loss8.csv',loss21, delimiter=',')

3

