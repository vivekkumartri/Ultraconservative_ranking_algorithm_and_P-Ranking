# -*- coding: utf-8 -*-
"""IMDb_implemetation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/188oaBRKxpXPbIVg1Af14j7ozuis_xP7V
"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
#import seaborn as sns
#import nltk
#from nltk.corpus import stopwords
#nltk.download('stopwords')
from sklearn.utils.extmath import safe_sparse_dot

imdb_sup_df=pd.read_csv("/home/22n0457/imdb_sup.csv")

imdb_sup_df.head(10)

imdb_sup_df=imdb_sup_df.iloc[:1000,:]

def text_clean(message):
  '''
  message = "#'This', is $string #with punction, <br /> @'html_tag' and actual message also!"

  return 'string punction html_tag actual message also'
  '''

  html_tag = '<br />'
  message = message.replace(html_tag,'')  # remove html tag
  message = re.sub(r'[^\w\s]', '', message)   # remove punctiation
  message = message.lower()
  message = [word for word in message.split() if word not in stopwords.words('english')]
  message = ' '.join(message)

  return message

test_str = "#'This', is $string #with punction, <br /> @'html_tag' and actual message also!"
text_clean(test_str)

imdb_sup_df['Review'] = imdb_sup_df['Review'].apply(text_clean)

imdb_sup_df

from nltk.stem.porter import PorterStemmer

porterstemmer = PorterStemmer()

def steming(message):
  return[porterstemmer.stem(word) for word in message.split()]

# 1-2 minute
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(tokenizer=steming) # here stemming is our user defined function as above

x = tfidf.fit_transform(imdb_sup_df['Review']).toarray()
y = imdb_sup_df['Rating'].values

# Commented out IPython magic to ensure Python compatibility.
# %cd '/path../'
import algorithm

loss1,w1=algorithm.widrowhoff(x,y,len(y),0.001,20)

np.savetxt('/home/22n0457/imdb/loss1.csv',loss1, delimiter=',')
np.savetxt('/home/22n0457/imdb/w1.csv',w1, delimiter=',')

loss2,w2,b2=algorithm.pranking(x,y,len(y),20)

np.savetxt('/home/22n0457/imdb/loss2.csv',loss2, delimiter=',')
np.savetxt('/home/22n0457/imdb/w2.csv',w2, delimiter=',')
np.savetxt('/home/22n0457/imdb/b2.csv',b2, delimiter=',')

loss3,m1=algorithm.uniformmulticlassalgo(x,y,len(y),20)

np.savetxt('/home/22n0457/imdb/loss3.csv',loss3, delimiter=',')

loss4,m2=algorithm.worstmulticlassalgo(x,y,len(y),20)

np.savetxt('/home/22n0457/imdb/loss4.csv',loss4, delimiter=',')

loss5,m3=algorithm.vimulticlassalgo(x,y,len(y),20)

np.savetxt('/home/22n0457/imdb/loss5.csv',loss5, delimiter=',')

loss6,m4=algorithm.mira(x,y,len(y),20)

np.savetxt('/home/22n0457/imdb/loss6.csv',loss6, delimiter=',')

loss7,w21,b21=algorithm.pranking_updated(x,y,len(y),20)

np.savetxt('/home/22n0457/imdb/loss7.csv',loss7, delimiter=',')

from sklearn.utils.extmath import safe_sparse_dot

label=range(1,len(np.unique(y))+1)
unique_y = np.sort(np.unique(y))
y = np.array([np.where(unique_y == labels)[0][0] + 1 for labels in y])

model = algorithm.OrdinalRegressionNetwork(input_size=np.shape(x)[1], hidden_size=2**7, output_size=int(len(np.unique(y))))
#loss=model.train(x, y, learning_rate=0.1, epochs=1000)
loss=model.train(x[:100], y[:100], learning_rate=0.1, epochs=1)

np.savetxt('/home/22n0457/imdb/loss8.csv',loss, delimiter=',')

loss

5

